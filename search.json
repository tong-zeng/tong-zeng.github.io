[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Tong Zeng",
    "section": "",
    "text": "Welcome to Machine Learning Blog\n\n\n\nNews\n\n\n\nThis is the first post in my Machine Learning blog series. Welcome!\n\n\n\nTong Zeng\n\n\nNov 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\nProbability\n\n\nNaive Bayes\n\n\nSupervised Learning\n\n\n\nUsing naive bayes for credit card fraud detection\n\n\n\nTong Zeng\n\n\nNov 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\nRegression\n\n\nDeep Learning\n\n\nSupervised Learning\n\n\n\nHouse prices prediction with linear and nonlinear models\n\n\n\nTong Zeng\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification\n\n\n\nClassification\n\n\nDeep Learning\n\n\nSupervised Learning\n\n\n\nTrain classification models for dog diseases prediction\n\n\n\nTong Zeng\n\n\nNov 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\nClustering\n\n\nUnsupervised Learning\n\n\n\nApplication of clustering for customer segmentation\n\n\n\nTong Zeng\n\n\nNov 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnomaly Detection\n\n\n\nAnomaly Detection\n\n\nUnsupervised Learning\n\n\n\nFinding unusual rare cases different from the majority\n\n\n\nTong Zeng\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "news/first-news/index.html",
    "href": "news/first-news/index.html",
    "title": "Tong Zeng",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "blogs/anomaly-detection/index.html",
    "href": "blogs/anomaly-detection/index.html",
    "title": "Anomaly Detection",
    "section": "",
    "text": "Image source: https://www.freepik.com",
    "crumbs": [
      "Blogs",
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "blogs/anomaly-detection/index.html#introduction",
    "href": "blogs/anomaly-detection/index.html#introduction",
    "title": "Anomaly Detection",
    "section": "Introduction",
    "text": "Introduction\nAnomaly detection (AD), also known as outlier detection, refers to the process of identifying patterns, events, or observations in a given dataset that deviate significantly from the norm or expected behavior. In other words, it involves finding instances that are unusual, rare, or different from the majority of the data. These instances are often referred to as anomalies, outliers, or novelties.\nThe primary goal of anomaly detection is to distinguish normal patterns from abnormal ones, which can have various applications in different domains. By identifying anomalies, one can uncover potential issues, errors, fraud, or unusual events that may require further investigation or action.\nThis concept has various applications in real world acorss different domains. For example:\n\nNetwork Security: Anomaly detection is applied to network traffic to identify abnormal patterns that may suggest a cyber attack or unauthorized access. Unusual spikes in data transfer or irregular access patterns could be indicative of security breaches.\nHealthcare Monitoring: In healthcare, anomaly detection can be employed to identify unusual patterns in patient data, such as vital signs or laboratory results. This can help in early detection of diseases or abnormalities.\nFraud Detection in Finance: Anomaly detection is commonly used in the financial sector to identify unusual patterns in transactions that may indicate fraudulent activities, such as credit card fraud or insider trading.\n…\n\n\n\n\n\n\n\n\nFigure 1: Illustration of Anomaly Detection Applications\n\n\n\nAnomaly detection is a crucial aspect of data analysis in many fields, helping to improve security, reduce fraud, enhance system reliability, and provide early warnings for potential issues.",
    "crumbs": [
      "Blogs",
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "blogs/anomaly-detection/index.html#methods",
    "href": "blogs/anomaly-detection/index.html#methods",
    "title": "Anomaly Detection",
    "section": "Methods",
    "text": "Methods\n\nCategorization of AD Algorithms\nConsidering the significant importance of anomaly detection, lots of efforts has been made to improve the performance of anomaly detection. Several techniques could be employed for anomaly detection, and the choice of method often depends on the characterizations of dataset and the specific requirements of the application. Here are some common techniques that could be utilized for anomaly detection:\nRule-Based Methods: These methods rely on the domain-specific rules that are generated from domain experts’ knowledge to define explicit rules or conditions to identify anomalies. In many domains, experts can define rules based on their understanding of the system or process. These rules may take the form of if-then statements that describe conditions under which a data point is considered anomalous. For example: If a financial transaction is significantly larger than typical transactions, consider it suspicious.\nStatistical Methods: These methods can be boardly categorized into absolute socre-based methods and relative score-based methods. - Absolute score-based methods: In absolute score-based methods, the anomaly score is calculated based on the absolute value of a statistical measure. For example, in Z-Score, the distance of a data point from the mean is expressed in terms of standard deviations. High absolute Z-scores indicate points far from the mean, and these are considered anomalies. - Relative score-based methods: In relative score-based methods, the anomaly score is interpreted relative to the distribution of the data. Instead of using fixed absolute thresholds, percentage-based thresholds may be employed. For example, identifying the top 1% or 5% of data points as anomalies.\n\n\n\n\n\n\n\n\n\n\n\n(a) Detecting Outliers with z-Scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Detecting Outliers with IQR\n\n\n\n\n\n\n\nFigure 2: Statistical Methods for Outliers Detection. source: https://www.scaler.com/\n\n\n\nClustering Methods: Clustering methods involve grouping data points into clusters based on their similarity and then identifying anomalies as data points that do not conform to these clusters. Commonly used methods such as DBSCAN, K-Means etc. - DBSCAN is a density-based clustering algorithm that groups together data points that are close to each other and have a sufficient number of neighbors. Points that do not belong to any cluster or are in sparser regions of the data are considered anomalies (noise). - K-Means partitions the dataset into a predetermined number of clusters (k) based on the mean of data points in each cluster. Data points that do not fit well into any cluster or are far from the cluster centers may be considered anomalies.\nMachine Learning Methods: Machine learning-based methods for anomaly detection involve training models to learn patterns in normal data and then identifying instances that deviate significantly from these learned patterns. For example, Isolation Forest is an ensemble method that works by isolating anomalies in a dataset. It does this by randomly selecting a feature and then creating a split between the minimum and maximum values of that feature. Anomalies are expected to be isolated with fewer splits.\nDeep Learning Methods: Deep learning-based methods for anomaly detection leverage neural networks, particularly deep architectures, to model complex patterns and representations in data. For example, Autoencoders are neural networks trained to encode input data into a compressed representation and then decode it back to the original form. Anomalies are identified by measuring the difference between the input and the reconstructed output. Instances with high reconstruction errors are considered anomalies.\nExperts often recommend considering the nature of the data and the specific goals of anomaly detection when choosing between these categories. There is no one-size-fits-all approach, and the choice may depend on factors such as the data distribution, the presence of outliers, and the desired trade-off between false positives and false negatives. Additionally, combining multiple methods or using hybrid approaches is common for improved robustness and accuracy in real-world applications.\n\n\nTypical AD Algorithms\nAs mentioned above, there are many anomaly detection algorithms available. In this post, we will choose two algorithms that are used more in sklearn for our anomaly detection experiments. They are described as follows:\n\nIsolation Forest\nThe Isolation Forest algorithm is an unsupervised method for detecting anomalies by leveraging the characteristics of anomalies, specifically their scarcity and distinctiveness. Since anomalies are both infrequent and different from the majority of data points, they are more prone to isolation. This algorithm individually isolates each data point and categorizes them as outliers or inliers based on the time it takes to separate them. The separation process is influenced by the number of points within the proximity of a given point. If an attempt is made to isolate a point that is clearly not an outlier, it will likely be surrounded by many points, making isolation challenging. Conversely, if the point is indeed an outlier, it will be isolated easily as it tends to be solitary in the dataset. An illustration of the algorithm is listed below:\n\n\n\n\n\n\nFigure 3: Illustration of Isolation Forest Method. source: https://doi.org/10.3390/rs12101678\n\n\n\n\n\nLocal Outlier Factor\nThe Local Outlier Factor (LOF) algorithm is an unsupervised approach for identifying anomalies by assessing the local density variation of a specific data point in relation to its neighboring points. This computation involves examining the density of a point’s neighbors and contrasting it with the density of subsequent neighboring points. In essence, LOF identifies outliers by detecting instances where the density surrounding an object significantly differs from the density around its neighboring points. LOF labels samples as outliers if they exhibit notably lower density compared to their neighboring points.\n\n\n\nEvaluation Metrics\nAnomaly detection can essentially be viewed as a binary classification task that distinguishes data points into two categories: normal and outliers. Therefore, common evaluation metrics in classification tasks can also be used for anomaly detection.\nIn this post, we will use the precision, recall and F_1 are as indicators for model performance evaluation.\nAccording to wikipedia, the definition of these metrics as following :\nPrecision is the fraction of relevant instances among the retrieved instances. Written as a formula:\n\n{\\displaystyle {\\text{Precision}}={\\frac {\\text{Relevant retrieved instances}}{\\text{All retrieved instances}}}}\n\nRecall is the fraction of relevant instances that were retrieved. Written as a formula:\n\n{\\displaystyle {\\text{Recall}}={\\frac {\\text{Relevant retrieved instances}}{\\text{All relevant instances}}}}\n\nF_1 score is the harmonic mean of the precision and recall. Written as a formula:\n\n{\\displaystyle F_{1}={\\frac {2}{\\mathrm {recall} ^{-1}+\\mathrm {precision} ^{-1}}}=2{\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}={\\frac {2\\mathrm {tp} }{2\\mathrm {tp} +\\mathrm {fp} +\\mathrm {fn} }}}\n\nwhere tp is the true positives, fp is the false positive, and fn is the false negative.",
    "crumbs": [
      "Blogs",
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "blogs/anomaly-detection/index.html#dataset",
    "href": "blogs/anomaly-detection/index.html#dataset",
    "title": "Anomaly Detection",
    "section": "Dataset",
    "text": "Dataset\nIn this section, we will load the libraries, analysis the dataset and get insights from the data distribution through data visualization. The procedures are listed below:\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\n\nLoad the Data\n\nnRowsRead = 1000\ndf = pd.read_csv('data/creditcard_10k.csv', delimiter=',', nrows = nRowsRead)\ndf\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n995\n751\n-0.654892\n0.608319\n1.585102\n-3.009429\n0.037593\n-1.954023\n1.335977\n-0.612858\n0.690254\n...\n-0.078527\n-0.064194\n-0.107350\n0.961776\n-0.067760\n-0.549465\n-0.232588\n-0.108261\n3.90\n0\n\n\n996\n752\n-2.101171\n-0.227365\n1.624668\n-0.291123\n1.902446\n-1.483921\n-0.275117\n0.085964\n-0.563098\n...\n-0.313782\n-0.804784\n-0.474101\n0.008102\n0.259725\n0.032376\n0.323580\n0.034622\n1.78\n0\n\n\n997\n753\n-1.248163\n0.315246\n3.708935\n0.687280\n-0.873071\n1.091287\n0.297707\n-0.633135\n1.102317\n...\n-0.824013\n0.057907\n-0.282351\n0.630774\n0.283506\n-0.204264\n0.097555\n-0.670480\n30.00\n0\n\n\n998\n755\n1.374134\n-1.767210\n-0.433352\n-2.229552\n0.331135\n3.924775\n-2.049947\n1.001403\n-1.183310\n...\n-0.252942\n-0.461534\n-0.030890\n0.997119\n0.384299\n-0.187538\n0.068817\n0.038009\n82.37\n0\n\n\n999\n755\n-2.497436\n1.402769\n0.184840\n-2.504117\n-0.111803\n-0.902909\n0.110183\n-3.655788\n2.231761\n...\n2.777155\n-0.664909\n0.594689\n0.330380\n0.064190\n-0.110533\n0.672165\n0.114739\n8.00\n0\n\n\n\n\n1000 rows × 31 columns\n\n\n\n\n\nDescriptive Statistics\n\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n\nNo Frauds 99.8 % of the dataset\nFrauds 0.2 % of the dataset\n\n\n\nfraud = len(df[df['Class'] == 1]) / len(df) * 100\nnofraud = len(df[df['Class'] == 0]) / len(df) * 100\nfraud_percentage = [nofraud, fraud]\nfig = plt.figure()\nplt.pie(fraud_percentage,labels = ['Fraud','No Fraud'], autopct='%1.1f%%', colors=['deepskyblue', 'tomato'])\nfig.patch.set_facecolor('white')\nfig.patch.set_alpha(0)\nplt.title('Number of Fraud Cases');\nplt.show()\n\n\n\n\n\n\n\n\n\n\nData Distribution\nAs we can see from the outputs above, the dataset is highly unbalanced, the frauds data points only accounts for 0.2% which makes the anomaly detection challenge compare to most supervised learning.\n\nplt.style.use('default')\nsns.pairplot(df[[\"Amount\", \"Time\"]], aspect=1.5, height=2.6, kind=\"scatter\", diag_kind=\"hist\")\nplt.rc('font', **{'size': 13})\nplt.show()\n\n\n\n\n\n\n\n\nThe Amount are highly skewed, but the data points are almost equally distributed over the time. We then examine the relationship between the variables by plotting the correlation heatmap as below:\n\nplt.style.use('default')\ndf_corr = df.corr()\nplt.figure(figsize=(8.8,7))\nsns.heatmap(df_corr, cmap=\"YlGnBu\") \nsns.set(font_scale=2,style='white')\n\nplt.rc('font', **{'size': 13})\nplt.title('Heatmap correlation', fontsize=13)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3D Visualization\nIn order to get a better understanding of the dataset, we can use PCA to transform the data into 3 dimensions and mark the outliers in the 3D space.\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom mpl_toolkits.mplot3d import Axes3D\n\nnum_df = df.columns[0:30]\noutliers = df.loc[df['Class']==1]\noutlier_index=list(outliers.index)\n\nscaler = StandardScaler()\nX = scaler.fit_transform(df[num_df])\npca_model = PCA(n_components=3)\nX_pca = pca_model.fit_transform(X)\n\n\nplt.style.use('default')\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], zs=X_pca[:, 2], s=3, lw=1, label=\"inliers\",c=\"deepskyblue\")\nax.scatter(X_pca[outlier_index,0],X_pca[outlier_index,1], X_pca[outlier_index,2], s=60, lw=2, marker=\"*\", c=\"tomato\", label=\"outliers\")\nax.legend(loc=2, prop={'size': 8})\nax.zaxis.set_tick_params(labelsize=8)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()\n\n\n\n\nFigure 4: Visualization of Outliers in 3D Space",
    "crumbs": [
      "Blogs",
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "blogs/anomaly-detection/index.html#experiments",
    "href": "blogs/anomaly-detection/index.html#experiments",
    "title": "Anomaly Detection",
    "section": "Experiments",
    "text": "Experiments\n\nPreprocessing\nWe first load the libraries, and prepare the data for modeling.\n\nfrom sklearn.metrics import classification_report,accuracy_score, confusion_matrix\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n\ndf = pd.read_csv('data/creditcard_10k.csv', delimiter=',', nrows = 10000)\ncolumns = df.columns.tolist()\ncolumns = [c for c in columns if c not in [\"Class\"]]\ntarget = \"Class\"\nstate = np.random.RandomState(666)\nX = df[columns]\nY = df[target]\nprint(X.shape)\nprint(Y.shape)\n\n(9999, 30)\n(9999,)\n\n\n\nFraud = df[df['Class']==1]\nValid = df[df['Class']==0]\noutlier_ratio = len(Fraud)/float(len(Valid))\nprint(f\"Fraud: {len(Fraud)}, Valid: {len(Valid)}, ratio:{outlier_ratio}\")\n\nFraud: 38, Valid: 9961, ratio:0.0038148780242947497\n\n\n\n\nTraining\nWe can now define the models. We are going to use OneClassSVM as the baseline, and focus on IsolationForest and LocalOutlierFactor for anomaly detection.\n\nclassifiers = {\n    \"Isolation Forest\": IsolationForest(\n        n_estimators=100, \n        max_samples=len(X),\n        contamination=outlier_ratio,\n        random_state=666,\n        verbose=0\n    ),\n    \"Local Outlier Factor\": LocalOutlierFactor(\n        n_neighbors=50, \n        algorithm='auto', \n        leaf_size=50, metric='minkowski',\n        p=2,\n        metric_params=None,\n        contamination=outlier_ratio\n    ),\n    \"Support Vector Machine\": OneClassSVM(\n        kernel='rbf', \n        degree=5, \n        gamma=0.1,nu=0.05,\n        max_iter=-1\n    )\n}\n\nTraining begins …\n\ndef fit_predict(classifier_name):\n    n_outliers = len(Fraud)\n    classifier = classifiers[classifier_name]\n    if classifier_name == \"Local Outlier Factor\":\n        y_pred = classifier.fit_predict(X)\n        scores_prediction = classifier.negative_outlier_factor_\n    elif classifier_name == \"Support Vector Machine\":\n        classifier.fit(X)\n        y_pred = classifier.predict(X)\n    else:    \n        classifier.fit(X)\n        scores_prediction = classifier.decision_function(X)\n        y_pred = classifier.predict(X)\n    # 0 for Valid, 1 for Fraud\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    print(f\"{classifier_name}: {n_errors} prediction errors\")\n    return y_pred\n\n\ny_pred_if = fit_predict(\"Isolation Forest\")\ny_pred_lof = fit_predict(\"Local Outlier Factor\")\ny_pred_svm = fit_predict(\"Support Vector Machine\")\n\nIsolation Forest: 51 prediction errors\nLocal Outlier Factor: 75 prediction errors\nSupport Vector Machine: 5529 prediction errors\n\n\nTraining completed! At first glance, SVM performs very bad, it has 5528 prediction errors which means the accuracy is less than 50%. The Isolation Forest and Local Outlier Factor are looks good, with accuracy more than 99%. However, the accuracy is not enough for assessment, because in some application, it is very costly to miss any outlier but it can tolerance to classify a normal data point as outlier. However, in other cases it might be very expensive to treat a normal data point as outlier, but wouldn’t be a big problem to miss a outlier.\n\n\nPerformance Evaluation\nIn this section, we will evaluate the model performance with the precision, recall and F_1 score, as long as a visualization of the confusion matrix.\n\ndef evaluate(y_pred):\n    #print(\"Accuracy Score:\")\n    #print(accuracy_score(Y,y_pred))\n    #print(\"Classification Report:\")\n    print(classification_report(Y,y_pred))\n    \ndef display_confusion_matrix(y_pred):\n    plt.style.use('default')\n    cm = confusion_matrix(Y,y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    fig, ax = plt.subplots(figsize=(1.1,1.1))\n    return disp.plot(ax=ax)\n\n\nevaluate(y_pred_lof)\ndisp = display_confusion_matrix(y_pred_lof)\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      9961\n           1       0.03      0.03      0.03        38\n\n    accuracy                           0.99      9999\n   macro avg       0.51      0.51      0.51      9999\nweighted avg       0.99      0.99      0.99      9999\n\n\n\n\n\n\n\n\n\n\n\nevaluate(y_pred_if)\ndisp = display_confusion_matrix(y_pred_if)\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      9961\n           1       0.33      0.34      0.34        38\n\n    accuracy                           0.99      9999\n   macro avg       0.67      0.67      0.67      9999\nweighted avg       0.99      0.99      0.99      9999\n\n\n\n\n\n\n\n\n\n\n\nevaluate(y_pred_svm)\ndisp = display_confusion_matrix(y_pred_svm)\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.45      0.62      9961\n           1       0.00      0.42      0.01        38\n\n    accuracy                           0.45      9999\n   macro avg       0.50      0.43      0.31      9999\nweighted avg       0.99      0.45      0.61      9999\n\n\n\n\n\n\n\n\n\n\nIn our dataset, there are 38 frauds in total. From the performance report and confusion matrix, we can infer that: local outlier factor only detects 1 fraud data point, while isolation forest predicts 12 frauds successfully. In this sense, isolation forest has much better performance.",
    "crumbs": [
      "Blogs",
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "blogs/anomaly-detection/index.html#discussion-and-conclusion",
    "href": "blogs/anomaly-detection/index.html#discussion-and-conclusion",
    "title": "Anomaly Detection",
    "section": "Discussion and Conclusion",
    "text": "Discussion and Conclusion\nIn this post, we introduced the anomaly detection and its application across different domains, then we discussed the categorization of AD Algorithms, followed by a detailed explaination of isolation forest and local outlier factor. After some exploratory data analysis and data preprocessing, we trained machine learning models to detect the outliers. Our performance evaluation shows that isolation forest has the best performance.\nAs expected, the OneClassSVM doesn’t perform well for outlier detection, as it is very sensitive to outliers. This algorithm is best suited for applications where the training set is not contaminated by outliers.\nThe isolation forest and local outlier factor are much better than OneClassSVM, and performs reasonably well. However, the isolation forest has slightly better performance and it also runs faster than local outlier factor in our experiments.\nIn summary, when choosing between isolation forest and Local local outlier factor, you should consider the nature of your data, the types of outliers you expect, and the computational resources available. It’s often beneficial to experiment with both algorithms on your specific dataset and assess their performance using appropriate evaluation metrics. Ensemble approaches that combine the strengths of both methods may also be considered for more robust outlier detection.",
    "crumbs": [
      "Blogs",
      "Anomaly Detection"
    ]
  },
  {
    "objectID": "blogs/probability/index.html",
    "href": "blogs/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Image source: https://www.freepik.com",
    "crumbs": [
      "Blogs",
      "Probability Theory and Random Variables"
    ]
  },
  {
    "objectID": "blogs/probability/index.html#introduction",
    "href": "blogs/probability/index.html#introduction",
    "title": "Probability Theory and Random Variables",
    "section": "Introduction",
    "text": "Introduction\n\nProbability Theory\nProbability theory is a branch of mathematics that deals with the quantification of uncertainty and randomness. It provides a framework for reasoning about uncertainty and making predictions in the presence of incomplete information.\nProbability is a measure of the likelihood that a particular event will occur. The set of all possible events is called Sample Space, denoted by S. All The probability of an event E, denoted by P(E), is a number between 0 and 1, where 0 indicates impossibility, 1 indicates certainty, and values in between represent degrees of likelihood. It satisfies three axioms:\n\nNon-negativity: P(E) \\ge 0 for any event E.\nNormalization: P(S) = 1, indicating that at least one of the possible outcomes must occur.\nAdditivity: For any mutually exclusive events E_1, E_2, \\cdots, the probability of their union is the sum of their individual probabilities: P(E_1 \\cup E_2 \\cup \\cdots ) = P(E_1) + P(E_2) + \\cdots.\n\n\n\nConditional Probability and Marginal Probability\nConditional probability is the probability of an event A occurring given that another event B has already occurred. It is denoted by P(A∣B) and is calculated using the formula:\nP(A|B)=\\frac{P(A \\cap B)}{P(B)}\nwhere P(A \\cap B) is the probability of both A and B occurring together, and P(B) is the probability of B occurring.\nMarginal probability refers to the probability of a specific event irrespective of the occurrence or non-occurrence of other events. It is obtained by summing or integrating probabilities over all possible values of the other events. For a single event A, the marginal probability is denoted by P(A). If A_1, A_2, \\cdots, A_n are mutually exclusive events whose union is the entire sample space S, then for any event B, the law of total probability states:\nP(B)=P(B \\cap A_1) + P(B \\cap A_2) + \\cdots + P(B \\cap A_n)\nConditional probability represents the likelihood of event A occurring under the condition that event B has occurred. Marginal probability provides the overall likelihood of a particular event without considering the influence of other events. Understanding conditional and marginal probabilities is crucial for analyzing and modeling random phenomena.\nConditional probability allows us to update probabilities based on additional information, while marginal probability provides the overall likelihood of specific events in the absence of additional conditions. The multiplication rule and Bayes’ theorem highlight the relationships between these two concepts in probabilistic reasoning.\n\n\nApplications\nProbability plays a crucial role in various aspects of machine learning. In machine learning, probability is not only a tool for modeling uncertainty but also a foundation for making informed decisions based on available data. It enables practitioners to quantify uncertainty, incorporate prior knowledge, and build models that can make predictions with associated confidence levels.\nBayesian methods, in particular, highlight the importance of updating beliefs as new data becomes available, contributing to a more flexible and adaptive learning process.",
    "crumbs": [
      "Blogs",
      "Probability Theory and Random Variables"
    ]
  },
  {
    "objectID": "blogs/probability/index.html#methodology",
    "href": "blogs/probability/index.html#methodology",
    "title": "Probability Theory and Random Variables",
    "section": "Methodology",
    "text": "Methodology\n\nBayes’ Theorem\nNaive Bayes is based on Bayes’ theorem, which is a fundamental concept in probability theory. Bayes’ theorem relates the conditional and marginal probabilities of random events. In the context of Naive Bayes, it helps in estimating the probability of a particular class given a set of features.\nThe formula for Bayes’ theorem is as follows:\nP(A|B)=\\frac{P(B|A)\\cdot P(A)}{P(B)}\nwhere P(A|B) is the probability of a class given the observed features, P(B|A) is the likelihood of observing the features given the class, P(A) is the prior probability of the class, P(B) is the probability of observing the features.\n\n\nNaive Bayes Algorithm\nIn the Naive Bayes algorithm, the goal is often to find the class label C that maximizes the posterior probability P(C∣X), where X is a set of observed features. Due to the independence assumption (naive assumption), this can be simplified as follows:\n\\\\ P(C∣X) \\propto P(C) \\cdot \\prod_{i=1}^{n} P(x_i|C)\nwhere P(C) is the prior probability of class C, P(x_i|C) is the likelihood of observing feature x_i given class C, \\prod_{i=1}^{n} P(x_i|C) represents the product of the likelihoods for all features.\nThe proportionality sign indicates that we are interested in the class that maximizes this expression. Finally, the class with the highest posterior probability is chosen as the predicted class.\nIn summary, Naive Bayes uses probability theory, specifically Bayes’ theorem, to estimate the likelihood of a particular class given observed features. The naive assumption of feature independence simplifies the computation, making it a powerful and computationally efficient algorithm for certain types of classification tasks, such as text classification.",
    "crumbs": [
      "Blogs",
      "Probability Theory and Random Variables"
    ]
  },
  {
    "objectID": "blogs/probability/index.html#datasets",
    "href": "blogs/probability/index.html#datasets",
    "title": "Probability Theory and Random Variables",
    "section": "Datasets",
    "text": "Datasets\n\nDescription\nIn this bolg, we consider the task of fraud detection thought the method of Naive Bayes.\nThe dataset comprises credit card transactions conducted by European cardholders in September 2013. Within this dataset, there were 492 instances of fraud among a total of 284,807 transactions spanning a two-day period.\nThe dataset exhibits significant imbalance, with frauds representing a mere 0.172% of the entire transaction volume, categorizing the positive class as relatively rare. It only contains numerical input variables resulting from a Principal Component Analysis (PCA) transformation. Due to the constraints of confidentiality, we are unable to furnish the original features and additional contextual information about the dataset.\nNotably, features V1 through V28 represent the principal components derived from PCA. The only exceptions are ‘Time’ and ‘Amount,’ which have not undergone PCA transformation. ‘Time’ signifies the elapsed seconds between each transaction and the initial transaction in the dataset, while ‘Amount’ denotes the transaction amount. The latter feature is particularly useful for example-dependent cost-sensitive learning. Lastly, the target variable, ‘Class’ will be 1 if it is fraud and 0 otherwise.\nIn order to speed up the compilation of the blog, we only includes 10,000 records of the dataset in the following analysis.\n\n\nData Preprocessing and Visualization\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix,auc,roc_auc_score\nfrom sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score\n\n\ndf = pd.read_csv(\"data/creditcard_10k.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\ncount\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n...\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n9999.000000\n\n\nmean\n5965.128713\n-0.241768\n0.281829\n0.906164\n0.263916\n-0.046353\n0.133134\n-0.071791\n-0.064802\n0.802379\n...\n-0.051994\n-0.152667\n-0.033292\n0.021271\n0.087125\n0.108154\n0.005533\n0.002912\n63.020562\n0.003800\n\n\nstd\n4472.712504\n1.521726\n1.308149\n1.159164\n1.441120\n1.182986\n1.307373\n1.077437\n1.259125\n1.155150\n...\n0.913856\n0.631114\n0.487832\n0.594425\n0.428188\n0.562819\n0.410886\n0.266261\n184.492872\n0.061533\n\n\nmin\n0.000000\n-27.670569\n-34.607649\n-15.496222\n-4.657545\n-32.092129\n-23.496714\n-26.548144\n-23.632502\n-6.329801\n...\n-11.468435\n-8.527145\n-15.144340\n-2.512377\n-2.577363\n-1.338556\n-7.976100\n-3.509250\n0.000000\n0.000000\n\n\n25%\n2072.500000\n-1.013135\n-0.208350\n0.412752\n-0.614489\n-0.643510\n-0.629975\n-0.542363\n-0.190784\n0.071116\n...\n-0.268135\n-0.549665\n-0.174123\n-0.327857\n-0.158137\n-0.328038\n-0.084479\n-0.015753\n5.000000\n0.000000\n\n\n50%\n4563.000000\n-0.372624\n0.288381\n0.944342\n0.219852\n-0.152524\n-0.152621\n-0.055776\n0.012791\n0.805293\n...\n-0.123286\n-0.136735\n-0.045811\n0.079956\n0.120865\n0.042879\n-0.004566\n0.015893\n15.950000\n0.000000\n\n\n75%\n10231.500000\n1.150869\n0.901632\n1.602707\n1.125501\n0.371301\n0.505448\n0.476175\n0.274582\n1.506342\n...\n0.032715\n0.247521\n0.081546\n0.410798\n0.359081\n0.476792\n0.120836\n0.077184\n50.890000\n0.000000\n\n\nmax\n15010.000000\n1.960497\n8.636214\n4.101716\n10.463020\n34.099309\n21.393069\n34.303177\n5.060381\n10.392889\n...\n22.588989\n4.534454\n13.876221\n3.200201\n5.525093\n3.517346\n8.254376\n4.860769\n7712.430000\n1.000000\n\n\n\n\n8 rows × 31 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9999 entries, 0 to 9998\nData columns (total 31 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Time    9999 non-null   int64  \n 1   V1      9999 non-null   float64\n 2   V2      9999 non-null   float64\n 3   V3      9999 non-null   float64\n 4   V4      9999 non-null   float64\n 5   V5      9999 non-null   float64\n 6   V6      9999 non-null   float64\n 7   V7      9999 non-null   float64\n 8   V8      9999 non-null   float64\n 9   V9      9999 non-null   float64\n 10  V10     9999 non-null   float64\n 11  V11     9999 non-null   float64\n 12  V12     9999 non-null   float64\n 13  V13     9999 non-null   float64\n 14  V14     9999 non-null   float64\n 15  V15     9999 non-null   float64\n 16  V16     9999 non-null   float64\n 17  V17     9999 non-null   float64\n 18  V18     9999 non-null   float64\n 19  V19     9999 non-null   float64\n 20  V20     9999 non-null   float64\n 21  V21     9999 non-null   float64\n 22  V22     9999 non-null   float64\n 23  V23     9999 non-null   float64\n 24  V24     9999 non-null   float64\n 25  V25     9999 non-null   float64\n 26  V26     9999 non-null   float64\n 27  V27     9999 non-null   float64\n 28  V28     9999 non-null   float64\n 29  Amount  9999 non-null   float64\n 30  Class   9999 non-null   int64  \ndtypes: float64(29), int64(2)\nmemory usage: 2.4 MB\n\n\n\nfig, ax = plt.subplots(1, 1)\nax.pie(df.Class.value_counts(),autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['dodgerblue','r'])\nplt.axis('equal')\nplt.ylabel('')\nplt.show()\n\n\n\n\n\n\n\n\n\ndf[\"Time_Hr\"] = df[\"Time\"]/3600\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex = True, figsize=(6,3))\nax1.hist(df.Time_Hr[df.Class==0],bins=48,color='tomato',alpha=0.5)\nax1.set_title('Genuine')\nax2.hist(df.Time_Hr[df.Class==1],bins=48,color='deepskyblue',alpha=0.5)\nax2.set_title('Fraud')\nplt.xlabel('Time (hrs)')\nplt.ylabel('# transactions')\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, (ax3,ax4) = plt.subplots(2,1, figsize = (6,3), sharex = True)\nax3.hist(df.Amount[df.Class==0],bins=50,color='tomato',alpha=0.5)\nax3.set_yscale('log') # to see the tails\nax3.set_title('Genuine') # to see the tails\nax3.set_ylabel('# transactions')\nax4.hist(df.Amount[df.Class==1],bins=50,color='deepskyblue',alpha=0.5)\nax4.set_yscale('log') # to see the tails\nax4.set_title('Fraud') # to see the tails\nax4.set_xlabel('Amount ($)')\nax4.set_ylabel('# transactions')\nplt.show()\n\n\n\n\n\n\n\n\n\ndf['scaled_Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\ndf = df.drop(['Amount'],axis=1)",
    "crumbs": [
      "Blogs",
      "Probability Theory and Random Variables"
    ]
  },
  {
    "objectID": "blogs/probability/index.html#experiments",
    "href": "blogs/probability/index.html#experiments",
    "title": "Probability Theory and Random Variables",
    "section": "Experiments",
    "text": "Experiments\n\nfrom sklearn.naive_bayes import GaussianNB\n\n\ndef split_data(df, drop_list):\n    df = df.drop(drop_list,axis=1)\n    print(df.columns)\n    #test train split time\n    from sklearn.model_selection import train_test_split\n    y = df['Class'].values #target\n    X = df.drop(['Class'],axis=1).values #features\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=42, stratify=y)\n\n    print(\"train-set size: \", len(y_train),\n      \"\\ntest-set size: \", len(y_test))\n    print(\"fraud cases in test-set: \", sum(y_test))\n    return X_train, X_test, y_train, y_test\n\n\ndef get_predictions(clf, X_train, y_train, X_test):\n    # create classifier\n    clf = clf\n    # fit it to training data\n    clf.fit(X_train,y_train)\n    # predict using test data\n    y_pred = clf.predict(X_test)\n    # Compute predicted probabilities: y_pred_prob\n    y_pred_prob = clf.predict_proba(X_test)\n    #for fun: train-set predictions\n    train_pred = clf.predict(X_train)\n    print('train-set confusion matrix:\\n', confusion_matrix(y_train,train_pred)) \n    return y_pred, y_pred_prob\n\n\ndef print_scores(y_test,y_pred,y_pred_prob):\n    print('test-set confusion matrix:\\n', confusion_matrix(y_test,y_pred)) \n    print(\"recall score: \", recall_score(y_test,y_pred))\n    print(\"precision score: \", precision_score(y_test,y_pred))\n    print(\"f1 score: \", f1_score(y_test,y_pred))\n    print(\"accuracy score: \", accuracy_score(y_test,y_pred))\n    print(\"ROC AUC: {}\".format(roc_auc_score(y_test, y_pred_prob[:,1])))\n\n\ndrop_list = []\nX_train, X_test, y_train, y_test = split_data(df, drop_list)\ny_pred, y_pred_prob = get_predictions(GaussianNB(), X_train, y_train, X_test)\nprint_scores(y_test,y_pred,y_pred_prob)\n\nIndex(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class',\n       'Time_Hr', 'scaled_Amount'],\n      dtype='object')\ntrain-set size:  7999 \ntest-set size:  2000\nfraud cases in test-set:  8\ntrain-set confusion matrix:\n [[7914   55]\n [   1   29]]\ntest-set confusion matrix:\n [[1967   25]\n [   1    7]]\nrecall score:  0.875\nprecision score:  0.21875\nf1 score:  0.35\naccuracy score:  0.987\nROC AUC: 0.9919992469879518",
    "crumbs": [
      "Blogs",
      "Probability Theory and Random Variables"
    ]
  },
  {
    "objectID": "blogs/probability/index.html#discussion-and-conclusions",
    "href": "blogs/probability/index.html#discussion-and-conclusions",
    "title": "Probability Theory and Random Variables",
    "section": "Discussion and Conclusions",
    "text": "Discussion and Conclusions\n\nDiscussion\nThe Naive Bayes algorithm has proven to be a powerful and efficient tool in various machine learning applications, particularly in classification tasks. Its simplicity, speed, and effectiveness make it well-suited for scenarios where computational resources are limited or where a quick and interpretable solution is desired.\nOne key strength of Naive Bayes lies in its probabilistic foundation. The algorithm leverages Bayes’ theorem to estimate the probability of a particular class given observed features. The assumption of feature independence, though naive, often proves to be a reasonable approximation in practice. This assumption significantly simplifies the computational complexity of the algorithm, allowing it to scale well to high-dimensional datasets.\n\n\nLimitations\nIt’s crucial to acknowledge the limitations of the Naive Bayes algorithm. The assumption of feature independence may not always hold in real-world datasets,(e.g., in the above example of fraud detection) and in such cases, more sophisticated models might be warranted. Furthermore, Naive Bayes is sensitive to the quality of input features, and the presence of irrelevant or redundant features can impact its performance. In cases where the assumption of independence is a significant concern, more advanced techniques such as ensemble methods or probabilistic graphical models might be explored. These approaches can capture more complex relationships between features and enhance the algorithm’s predictive capabilities.\n\n\nConclusions\nWhile Naive Bayes may not be the optimal choice for every machine learning task, its simplicity and efficiency make it a valuable option in specific contexts. As with any algorithm, careful consideration of the underlying assumptions and characteristics of the dataset is essential. Naive Bayes serves as a foundational tool in the machine learning toolbox, providing a baseline for comparison and a quick solution for certain types of classification problems.",
    "crumbs": [
      "Blogs",
      "Probability Theory and Random Variables"
    ]
  },
  {
    "objectID": "blogs/regression/index.html",
    "href": "blogs/regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Image source: https://www.freepik.com",
    "crumbs": [
      "Blogs",
      "Linear and Nonlinear Regression"
    ]
  },
  {
    "objectID": "blogs/regression/index.html#introduction",
    "href": "blogs/regression/index.html#introduction",
    "title": "Linear and Nonlinear Regression",
    "section": "Introduction",
    "text": "Introduction\nIn this blog, we’re going to solve the House Prices Prediction problem with linear and nonlinear regression models. Regression is one of the most important machine learning (ML) algorithm, it is very straightforward and intuitive. It is usually the first algorithm we encounter in our machine learning journey, so let’s conquer it together!\nWe’re going to cover the following regression models in this post:\n\nLinear Models\n\nLinear Regression\nRidge Regression\nLasso Regression\nElastic Net Regression\n\nNonlinear Models\n\nPolynomial Regression\nArtificial Neural Network",
    "crumbs": [
      "Blogs",
      "Linear and Nonlinear Regression"
    ]
  },
  {
    "objectID": "blogs/regression/index.html#data",
    "href": "blogs/regression/index.html#data",
    "title": "Linear and Nonlinear Regression",
    "section": "Data",
    "text": "Data\nThe dataset used in this blog is downloaded from Kaggle USA Housing. The whole dataset is about 5000 rows and 7 columns stored in the csv format, a local downloaded copy could be found as USA_Housing.csv file.\n\nExploratory Data Analysis\nWe first conduct some exploratory data analysis. We’re going to use the pandas library to load and process data, and use the matplotlib and seaborn library for data visualizations.\n\nImport Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport hvplot.pandas\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n\n\nLoad Data\n\nhousing_df = pd.read_csv('data/USA_Housing.csv')\nhousing_df.head()\n\n\n\nTable 1: Samples of the dataset\n\n\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\nAddress\n\n\n\n\n0\n79545.458574\n5.682861\n7.009188\n4.09\n23086.800503\n1.059034e+06\n208 Michael Ferry Apt. 674\\nLaurabury, NE 3701...\n\n\n1\n79248.642455\n6.002900\n6.730821\n3.09\n40173.072174\n1.505891e+06\n188 Johnson Views Suite 079\\nLake Kathleen, CA...\n\n\n2\n61287.067179\n5.865890\n8.512727\n5.13\n36882.159400\n1.058988e+06\n9127 Elizabeth Stravenue\\nDanieltown, WI 06482...\n\n\n3\n63345.240046\n7.188236\n5.586729\n3.26\n34310.242831\n1.260617e+06\nUSS Barnett\\nFPO AP 44820\n\n\n4\n59982.197226\n5.040555\n7.839388\n4.23\n26354.109472\n6.309435e+05\nUSNS Raymond\\nFPO AE 09386\n\n\n\n\n\n\n\n\n\n\nAs we can see in the Table 1, there are 6 numeric columns represents the average area income, house age, number of rooms, number of bedrooms, area population and price, respectively. There is only 1 textual column, keeps the house address.\nOur goal is modeling the house price, so it should be the dependent variable in the regressions, and all the other columns should be treated as the independent variable.\nWe now have a quick examination of the descriptive statistics.\n\nhousing_df.describe()\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\n\n\n\n\ncount\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5000.000000\n5.000000e+03\n\n\nmean\n68583.108984\n5.977222\n6.987792\n3.981330\n36163.516039\n1.232073e+06\n\n\nstd\n10657.991214\n0.991456\n1.005833\n1.234137\n9925.650114\n3.531176e+05\n\n\nmin\n17796.631190\n2.644304\n3.236194\n2.000000\n172.610686\n1.593866e+04\n\n\n25%\n61480.562388\n5.322283\n6.299250\n3.140000\n29403.928702\n9.975771e+05\n\n\n50%\n68804.286404\n5.970429\n7.002902\n4.050000\n36199.406689\n1.232669e+06\n\n\n75%\n75783.338666\n6.650808\n7.665871\n4.490000\n42861.290769\n1.471210e+06\n\n\nmax\n107701.748378\n9.519088\n10.759588\n6.500000\n69621.713378\n2.469066e+06\n\n\n\n\n\n\n\nIn order to have better insights on the variables, we further visualize the data distributions as follows.\n\nsns.pairplot(housing_df)\n\n\n\n\n\n\n\nFigure 1: The data distribution of variables\n\n\n\n\n\n\nAs Table 1 shows, most of our variables are normal distributed, except for the average number of bedrooms, there is clearly some linear trends between the dependent variable Price and other variables.\n\nhousing_df.hvplot.hist('Price')\nplt.show()\n\nWe further examine the relations between the dependent variable, for instance, we plot the relationship between Price and Avg. Area Income interactively as below.\n\nhousing_df.hvplot.scatter(x='Avg. Area Income', y='Price')\nplt.show()\n\n\n\nCorrelation Coefficient\n\naxes_arr = sns.heatmap(\n  housing_df[[\n    col for col in housing_df.columns if col not in [\"Address\",\"Price\"]\n    ]].corr()\n  )\nplt.show()\n\n\n\n\n\n\n\nFigure 2: The plot of Correlation Coefficient\n\n\n\n\n\nAs it is clearly shown in Figure 2, the Avg. Area Income is most correlated with Price, followed by Avg. Area House Age, Area Population, Avg. Area Number of Rooms. The variable Avg. Area Number of Bedrooms has the smallest correlation score at 0.17. Thus, we will include all these variables in the modeling section.\n\n\n\nPreprocessing\nIn order to fully exploit the linear regression, especially for better interpretability, we have to check if our data satisfies some criteria for linear regression application. For example:\n\nLinear Assumption. The linear regression models assume there are linear relationship between the dependent variable and independent variables.\nNoise Removal. The noises (i.e., missing value, outliers) in data could potentially impair the model performance, we have to remove these noise before modeling.\nGaussian Distribution. Linear regression works well with data comply with gaussian distribution. For those not, we have to apply data transformation techniques (e.g., log transform, BoxCox) to make the data more gaussian distributed.\nVariable Scaling. If variable values are at different magnitudes, we need to scale them to the same magnitudes by using standardization or normalization.\n\nWe have already tested the linear assumption and gaussian distribution in the exploratory data analysis section. And the data is relatively clean, noise is not a problem, however, the value of variables are at different magnitude, we will perform re-scaling before modeling steps.\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([\n    ('std_scalar', StandardScaler())\n])\n\nX = housing_df[[\n    col for col in housing_df.columns if col not in [\"Address\",\"Price\"] \n    ]]\ny = housing_df['Price'].to_numpy()\n\nX_scaled = pipeline.fit_transform(X)\n\npd.DataFrame(X_scaled).describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\ncount\n5.000000e+03\n5.000000e+03\n5.000000e+03\n5.000000e+03\n5.000000e+03\n\n\nmean\n-2.167155e-17\n4.263256e-18\n-4.135359e-16\n-2.088996e-16\n-3.041123e-16\n\n\nstd\n1.000100e+00\n1.000100e+00\n1.000100e+00\n1.000100e+00\n1.000100e+00\n\n\nmin\n-4.765584e+00\n-3.361975e+00\n-3.730214e+00\n-1.605598e+00\n-3.626413e+00\n\n\n25%\n-6.664724e-01\n-6.606490e-01\n-6.846171e-01\n-6.817833e-01\n-6.810902e-01\n\n\n50%\n2.075434e-02\n-6.852312e-03\n1.502401e-02\n5.564767e-02\n3.616311e-03\n\n\n75%\n6.756386e-01\n6.794590e-01\n6.742146e-01\n4.122077e-01\n6.748621e-01\n\n\nmax\n3.670725e+00\n3.572745e+00\n3.750297e+00\n2.041039e+00\n3.371219e+00\n\n\n\n\n\n\n\n\n\nData Split\nBefore the modeling steps, we first the dataset into train, validation and test split. We will use sklearn to perform this task.\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=666)\n\n\nprint(f\"The size of train split: {len(X_train)}\")\nprint(f\"The size of test split: {len(X_test)}\")\n\nThe size of train split: 4000\nThe size of test split: 1000",
    "crumbs": [
      "Blogs",
      "Linear and Nonlinear Regression"
    ]
  },
  {
    "objectID": "blogs/regression/index.html#modeling",
    "href": "blogs/regression/index.html#modeling",
    "title": "Linear and Nonlinear Regression",
    "section": "Modeling",
    "text": "Modeling\nIn this section, we will first introduce the model evaluation metrics and then modeling the data using various of regression algorithms.\n\nEvaluation\nThere are several popular evaluation metrics which are widely used in regression problem.\n\nRMSE\nRoot Mean Squared Error (RMSE) is the square root of the mean of the squared errors:\n\n\\operatorname{RMSE}= \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i} - y_i)^2}\n\nwhere,\n\nx_i = independent variable of each data point\ny_i = dependent variable of each data point\n\\bar{y} = mean of y value\n\\hat{y_i} = the predict y value base on x\n\n\n\nR-Squared (R²) Score\nR^2 describes the proportion of dependent variance explained by the regression model1. It could be denoted by the following equation.\n\n\\operatorname{R^2} = \\frac{SSR}{SST}\n\nwhere,\n\nSSR = \\sum_{i=1}^n \\left(\\hat{y_i} - \\bar{y}\\right)^2\n\\operatorname{SST} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2\n\n\n\nEvaluation using Sklearn\n\nfrom sklearn import metrics\n\ndef print_evaluate(true, predicted):  \n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return rmse, r2_square",
    "crumbs": [
      "Blogs",
      "Linear and Nonlinear Regression"
    ]
  },
  {
    "objectID": "blogs/regression/index.html#linear-models",
    "href": "blogs/regression/index.html#linear-models",
    "title": "Linear and Nonlinear Regression",
    "section": "Linear Models",
    "text": "Linear Models\n\nLinear Regression\nWe now train a linear regression model using sklearn library.\n\nTraining\n\nfrom sklearn.linear_model import LinearRegression\nlinear_md = LinearRegression()\nlinear_md.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nAfter the fit, we completed the model train. The next step is verify the fitting result and evaluate the model performance.\n\n\nInterpreting\nWe can access the model intercept and coefficients using the sklearn API:\n\nprint(f\"The intercept is: {linear_md.intercept_}\")\ncoef_df = pd.DataFrame(linear_md.coef_, X.columns, columns=['Coefficient'])\ncoef_df\n\nThe intercept is: 1232604.4076491522\n\n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nAvg. Area Income\n230982.846988\n\n\nAvg. Area House Age\n162799.384625\n\n\nAvg. Area Number of Rooms\n122310.560876\n\n\nAvg. Area Number of Bedrooms\n2376.796391\n\n\nArea Population\n151198.049691\n\n\n\n\n\n\n\n\n\nPrediction\nPredict using the model\n\npred = linear_md.predict(X_test)\npd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')\n\n\n\n\n\n  \n\n\n\n\nThe residual histogram is plotted as following:\n\npd.DataFrame({'Error Values': (y_test - pred)}).hvplot.kde()\n\n\n\n\n\n  \n\n\n\n\n\n\nPerformance\n\npred_train = linear_md.predict(X_train)\npred_test = linear_md.predict(X_test)\nprint(f\"The train set performance:\\n\")\nprint_evaluate(y_train, pred_train)\nprint(f\"The test set performance:\\n\")\nprint_evaluate(y_test, pred_test)\n\nThe train set performance:\n\nRMSE: 100782.52384490197\nR2 Square 0.9178558419818683\n__________________________________\nThe test set performance:\n\nRMSE: 102440.81503100201\nR2 Square 0.9184591335733732\n__________________________________\n\n\n\n\n\nRidge Regression\nRidge regression is a type of regularized linear regression, it imposes a penalty on the size of the coefficients, it could be expressed using the equation2:\n\n\\min_{w} || X w - y||_2^2 + \\alpha ||w||_2^2\n\nThe parameter \\alpha controls the amount of the penalty, the larger the \\alpha, the more penalty is imposed, the coefficient is more shrink to 0.\nSimilar to linear regression, we do modeling and evaluation using the ridge regression as following:\n\nfrom sklearn.linear_model import Ridge\nridge_md = Ridge(alpha = 20,  solver='cholesky',  tol=0.0001, random_state=666)\nridge_md.fit(X_train, y_train)\n\nRidge(alpha=20, random_state=666, solver='cholesky')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RidgeRidge(alpha=20, random_state=666, solver='cholesky')\n\n\n\npred_train = ridge_md.predict(X_train)\npred_test = ridge_md.predict(X_test)\nprint(f\"The train set performance:\\n\")\nprint_evaluate(y_train, pred_train)\nprint(f\"The test set performance:\\n\")\nprint_evaluate(y_test, pred_test)\n\nThe train set performance:\n\nRMSE: 100798.00734362529\nR2 Square 0.9178305999732655\n__________________________________\nThe test set performance:\n\nRMSE: 102413.21431785727\nR2 Square 0.9185030668996456\n__________________________________\n\n\n\n\nLasso Regression\nRidge regression is another type of regularized linear regression, specifically, using L1 regularization. It adds the absolute value of the coefficient to the loss function as the penalty. Mathematically, it could be expressed using the equation3:\n\n\\min_{w} { \\frac{1}{2n_{\\text{samples}}} ||X w - y||_2 ^ 2 + \\alpha ||w||_1}\n\nThe parameter \\alpha is a constant which controls the amount of the penalty, the ||w||_1 is the \\ell_1-norm of the coefficients.\nLasso Regression modeling and performance evaluation is performed bellow:\n\nfrom sklearn.linear_model import Lasso\n\nlasso_md = Lasso(alpha = 0.08, selection='random', positive=True, random_state=888)\nlasso_md.fit(X_train, y_train)\n\nLasso(alpha=0.08, positive=True, random_state=888, selection='random')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.08, positive=True, random_state=888, selection='random')\n\n\n\npred_train = lasso_md.predict(X_train)\npred_test = lasso_md.predict(X_test)\nprint(f\"The train set performance:\\n\")\nprint_evaluate(y_train, pred_train)\nprint(f\"The test set performance:\\n\")\nprint_evaluate(y_test, pred_test)\n\nThe train set performance:\n\nRMSE: 100782.52384504011\nR2 Square 0.917855841981643\n__________________________________\nThe test set performance:\n\nRMSE: 102440.81373676285\nR2 Square 0.9184591356337508\n__________________________________\n\n\n\n\nElastic Net Regression\nElastic net regression is essentially a combination of Ridge and Lasso regression. As it adds two regularization term: L2 and L1 regularization which is used in Ridge and Lasso regression respectively. The trade-off between Ridge and Lasso allows elastic net regression picks up the advantages of these two models. The lost function is written as4:\n\n\\min_{w}{\\frac{1}{2n_{samples}} \\big|\\big|X w - y\\big|\\big|_2 ^ 2 + \\alpha \\rho \\big|\\big|w\\big|\\big|_1 + \\frac{\\alpha(1-\\rho)}{2} \\big|\\big|w\\big|\\big|_2 ^ 2}\n\nThe parameters alpha \\alpha and l1_ratio \\rho could be decided by cross-validation.\n\nfrom sklearn.linear_model import ElasticNet\n\nenet_md = ElasticNet(alpha=0.08, l1_ratio=0.9, selection='random', random_state=666)\nenet_md.fit(X_train, y_train)\n\nElasticNet(alpha=0.08, l1_ratio=0.9, random_state=666, selection='random')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ElasticNetElasticNet(alpha=0.08, l1_ratio=0.9, random_state=666, selection='random')\n\n\n\npred_train = enet_md.predict(X_train)\npred_test = enet_md.predict(X_test)\nprint(f\"The train set performance:\\n\")\nprint_evaluate(y_train, pred_train)\nprint(f\"The test set performance:\\n\")\nprint_evaluate(y_test, pred_test)\n\nThe train set performance:\n\nRMSE: 100821.89673708729\nR2 Square 0.9177916466289507\n__________________________________\nThe test set performance:\n\nRMSE: 102411.86137925804\nR2 Square 0.9185052201299478\n__________________________________",
    "crumbs": [
      "Blogs",
      "Linear and Nonlinear Regression"
    ]
  },
  {
    "objectID": "blogs/regression/index.html#nonlinear-models",
    "href": "blogs/regression/index.html#nonlinear-models",
    "title": "Linear and Nonlinear Regression",
    "section": "Nonlinear Models",
    "text": "Nonlinear Models\nAll the model introduce above are belong to linear category. However, linear models is not designed to capture nonlinear trends in the data. If the relationship between dependent variable and independent variables are not linear, we have to apply nonlinear models. Among various options, polynomial regression and artificial neural network are the popular ones.\n\nPolynomial Regression\nPolynomial regression is a extension to linear regression by introducing polynomial terms. Recall the linear regression model looks like5:\n\n\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2\n\nHowever, we can add quadratic term to the previous equation to fit a parabola instead of a plane, then it looks like:\n\n    \\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\n\nWe can new image creating a set of features as:\n\nz = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]\n\nThen, we combine the features and coefficients linearly, and get the following:\n\n\\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\n\n\nModeling\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_md = PolynomialFeatures(degree=2)\n\nX_train_2_d = poly_md.fit_transform(X_train)\nX_test_2_d = poly_md.transform(X_test)\n\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train_2_d, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nPerformance\n\npred_train = linear_reg.predict(X_train_2_d)\npred_test = linear_reg.predict(X_test_2_d)\nprint(f\"The train set performance:\\n\")\nprint_evaluate(y_train, pred_train)\nprint(f\"The test set performance:\\n\")\nprint_evaluate(y_test, pred_test)\n\nThe train set performance:\n\nRMSE: 100640.24459847226\nR2 Square 0.9180876115116245\n__________________________________\nThe test set performance:\n\nRMSE: 102720.10827633881\nR2 Square 0.918013903645979\n__________________________________\n\n\n\n\n\nArtificial Neural Network\nArtificial Neural Network is a very powerful model to fit linear and nonlinear data. In this section, we will use pytorch framework to build a artificial neural network to predict the house prices. Similar to previous models, we start from build the network, then train the models and report the performance in the end.\n\nNetwork Construction\nImport the libraries:\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nDefine the HousePriceDataset by extending pytorch’s Dataset class, it is simply convert the numpy array to pytorch tensor.\n\nclass HousePriceDataset(Dataset):\n  def __init__(self, X, Y):\n    self.X = torch.tensor(X, dtype=torch.float32)\n    self.y = torch.tensor(Y, dtype=torch.float32)\n\n  def __len__(self):\n    return len(self.X)\n\n  def __getitem__(self, index):\n    return (self.X[index], self.y[index])\n\nWe new create the DataLoader objects, which load the data in batch to feed into the neural network.\n\ntrain_ds = HousePriceDataset(X_train, y_train)\ntest_ds = HousePriceDataset(X_test, y_test)\ntrainloader = DataLoader(train_ds, batch_size=32, shuffle=True,\n                         num_workers=2, drop_last=True)\ntestloader = DataLoader(test_ds, batch_size=32, shuffle=True, num_workers=2,\n                        drop_last=True)\n\nThe code below creates the neural network. It consist of several feed-forward layer with dropout regularization and use RELU as activate function. Each layer is like a linear regression, and the activate function map the output into the nonlinear space. That is why neural network could be used as a nonlinear regressor.\n\nclass MLP(nn.Module):\n  def __init__(self):\n    super(MLP, self).__init__()\n    self.layers = nn.Sequential(\n        nn.Linear(5, 64),\n        nn.Dropout(0.2),\n        nn.ReLU(),\n        nn.Linear(64, 128),\n        nn.Dropout(0.2),\n        nn.ReLU(),\n        nn.Linear(128, 64),\n        nn.Dropout(0.2),\n        nn.ReLU(),\n        nn.Linear(64, 1)\n    )\n  def forward(self, x):\n    return self.layers(x)\n\n\n\nTraining\nWe first implement the evaluate function which loads the test data, pass it to the pre-trained model and reports the MSE and R^2 as performance metrics.\n\ndef evaluate(model):\n    pred_list = []\n    label_list = []\n    loss_list = []\n    loss_function = nn.MSELoss()\n    model.eval()\n    for i, data in enumerate(testloader):\n        # Get inputs\n        inputs, targets = data\n        # Perform forward pass\n        outputs = model(inputs)\n        outputs = outputs.squeeze(-1)\n        loss = loss_function(outputs, targets)\n        loss_list.append(loss.item())\n        pred_list.append(outputs.detach().numpy())\n        label_list.append(targets.numpy())\n\n    avg_loss = sum(loss_list) / len(testloader)\n    pred_np = np.vstack(pred_list)\n    label_np = np.vstack(label_list)\n    avg_r2 = metrics.r2_score(pred_np, label_np)\n    return (avg_loss, avg_r2)\n\nThe then implements the train function to manage the whole training process, it initialize the neural network, data loader, loss function and the optimizer. Data is passed forward in the network and errors are propagated backward in the network.\n\ndef train():\n    mlp = MLP()\n    torch.manual_seed(666)\n    trainloader = DataLoader(train_ds, batch_size=32, shuffle=True,\n                             num_workers=2)\n    loss_function = nn.MSELoss()\n    optimizer = torch.optim.Adam(mlp.parameters(), lr=6e-4)\n    n_epochs = 150\n    mlp.train()\n    train_loss_list = []\n    train_r2_list = []\n    test_loss_list = []\n    test_r2_list = []\n    for epoch in range(n_epochs):\n        pred_list = []\n        label_list = []\n        loss_list = []\n        for i, data in enumerate(trainloader):\n            inputs, targets = data\n            # Zero the gradients\n            optimizer.zero_grad()\n            # Perform forward pass\n            outputs = mlp(inputs)\n            outputs = outputs.squeeze(-1)\n            # Compute loss\n            loss = loss_function(outputs, targets)\n            # Perform backward pass\n            loss.backward()\n            # Perform optimizationd\n            optimizer.step()\n            # Print statistics    \n            pred_list.append(outputs.detach().numpy())\n            label_list.append(targets.numpy())\n            loss_list.append(loss.item())\n\n        avg_loss = sum(loss_list) / len(trainloader)\n        pred_np = np.vstack(pred_list)\n        label_np = np.vstack(label_list)\n        avg_r2 = metrics.r2_score(pred_np, label_np)\n        train_loss_list.append(avg_loss)\n        train_r2_list.append(avg_r2)\n        test_loss, test_r2 = evaluate(mlp)\n        test_loss_list.append(test_loss)\n        test_r2_list.append(test_r2)\n        print(f\"epoch: {epoch}, test_loss: {test_loss}, test_r2: {test_r2}\")\n    print('Training process has finished.')\n    return mlp, train_loss_list, train_r2_list, test_loss_list, test_r2_list\n\nStart the training process:\n\nmodel, train_loss_list, train_r2_list, test_loss_list, test_r2_list = train()\n\n\n\nEvaluation\nSimilar to previous model, we reports the RMSE and R^2 of the test set as the evaluation metrics:\n\ntest_loss, test_r2 = evaluate(model)\nprint(f\"The test set performance:\\n\")\nrmse = np.sqrt(test_loss)\nprint('RMSE:', rmse)\nprint('R2 Square', test_r2)\nprint('__________________________________')\n\nThe test set performance:\n\nRMSE: 102623.80293089902\nR2 Square 0.9047423651342836\n__________________________________\n\n\n\ntrain_loss = train_loss_list\nval_loss = test_loss_list\ntrain_r2 = train_r2_list\nval_r2 = test_r2_list\n\nFurther, we can visualize the the RMSE of the train and test split during the whole training process, which is shown as follows:\n\n# Draw loss plot\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (6,6))\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.xlabel('Batch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss')\nplt.show()\n\n\n\n\n\n\n\n\nThe R^2 of the train and test split during the whole training process is shown as follows:\n\n# Draw accuracy plot\nplt.figure(figsize = (6,6))\nplt.plot(train_r2[10:], label='Training Accuracy')\nplt.plot(val_r2[10:], label='Validation Accuracy')\nplt.xlabel('Batch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy')\nplt.show()",
    "crumbs": [
      "Blogs",
      "Linear and Nonlinear Regression"
    ]
  },
  {
    "objectID": "blogs/regression/index.html#discussion-and-conclusion",
    "href": "blogs/regression/index.html#discussion-and-conclusion",
    "title": "Linear and Nonlinear Regression",
    "section": "Discussion and Conclusion",
    "text": "Discussion and Conclusion\nIn this blog, we applied 4 linear regression models and 2 nonlinear regression models in predicting the house price. Although trained on the same training set, these models vary in performance and have different characteristics in training.\nLinear or nonlinear regression should I use? The choice between linear regression and nonlinear regression depends on the underlying relationship between the independent and dependent variables in your dataset. Here are some guidelines can be summarized from our previous experiments:\nLinearity of the Relationship:\n\nUse linear regression when the relationship between the independent and dependent variables appears to be linear. Linear regression models assume a straight-line relationship, and they are suitable when the data points seem to follow a linear pattern.\nUse nonlinear regression when the relationship is not linear. If the data exhibits a curve, exponential growth, decay, or any other nonlinear pattern, a nonlinear regression model may be more appropriate.\n\nModel Complexity:\n\nLinear regression models are simpler and easier to interpret. If a linear relationship adequately describes the data, it’s often preferable due to its simplicity.\nNonlinear regression models can capture more complex relationships but may be harder to interpret. Be cautious about overfitting, where a model captures noise in the data rather than the underlying pattern.\n\nAs the house price data we used has a simple linear pattern, so, the performance is has no big difference than the nonlinear models. But it is much simple to implement and requires much less time and resource for training. In this kind of scenario, linear regression could be a good choice.\nIn summary, the decision to use linear or nonlinear regression depends on the characteristics of your data and the underlying relationship between variables.",
    "crumbs": [
      "Blogs",
      "Linear and Nonlinear Regression"
    ]
  },
  {
    "objectID": "blogs/regression/index.html#footnotes",
    "href": "blogs/regression/index.html#footnotes",
    "title": "Linear and Nonlinear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Coefficient_of_determination↩︎\nhttps://scikit-learn.org/stable/modules/linear_model.html#ridge-regression↩︎\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso↩︎\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet↩︎\nhttps://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions↩︎",
    "crumbs": [
      "Blogs",
      "Linear and Nonlinear Regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tong Zeng",
    "section": "",
    "text": "My name is Tong Zeng and I’m a Researcher and Machine Learning Engineer specializing in Science of Science and Applied Natural Language Processing \n          with a broad scientific background that ranges from economics, information science and computer science.\n          I have many years of experience working in the industry as a software development engineer and machine learning engineer.\n          \n            Know more\n            Contact me\n          \n        \n        \n        \n        \n          \n        \n      \n        \n  \n\n\n\n\n\nFEATURED PUBLICATIONS\n\n\nUnveiling Our Best Research Works and Beyond\n\n\n\n\n\n\n\n\n\n\n\n\nAssigning credit to scientific datasets using article citation networks\n\n\n\nScience of Science\n\n\nNetwork Science\n\n\nDATARANK\n\n\n\nDataRank - an algorithm to properly track and assign credit to datasets in citation network.\n\n\n\nTong Zeng, Longfeng Wu, Sarah Bratt, Daniel E Acuna\n\n\nJan 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models\n\n\n\nScience of Science\n\n\nDeep learning\n\n\n\nPredict whether a sentence needs citations using Att-BiLSTM Networks, Logistic Regression and Random Forest.\n\n\n\nTong Zeng, Daniel E Acuna\n\n\nMar 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDead Science: Most Resources Linked in Biomedical Articles Disappear in Eight Years\n\n\n\nScience of Science\n\n\nLogistic Regression\n\n\n\nModeling the resources decay in scientific publications.\n\n\n\nTong Zeng, Alain Shema, Daniel E Acuna\n\n\nMar 13, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding datasets in publications: the Syracuse University approach\n\n\n\nScience of Science\n\n\nDeep learning\n\n\n\nExtract dataset in publications using bi-LSTM-CRF network\n\n\n\nTong Zeng, Daniel E Acuna\n\n\nDec 31, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarge-scale author name disambiguation using approximate network structures\n\n\n\nScience of Science\n\n\nNetwork Science\n\n\nDATARANK\n\n\n\nAuthor name disambiguation at large-scale using pyspark.\n\n\n\nTong Zeng, Daniel E Acuna\n\n\nJul 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGotFunding: A grant recommendation system based on scientific articles\n\n\n\nScience of Science\n\n\nRecommendation System\n\n\n\nRecommend grant opportunity based on publication history\n\n\n\nTong Zeng, Daniel E Acuna\n\n\nOct 22, 2020\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n  \n    \n      \n        \n          PUBLICATION LIST\n        \n        Publications I Have Recently published\n      \n      \n\n\n  \n      JOURNAL ARTICLES\n\n    \n        \n        \n            \n            \n                 Determinants of diminishing returns on NIH-funded projects (under review)\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Daniel E Acuna\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                            \n                            \n                             2024\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n\n                        \n\n                        \n                    \n                \n                        \n        \n        \n    \n\n    \n        \n        \n            \n            \n                 Predicting the longevity of resources shared in scientific publications (to be appeared)\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    Daniel E Acuna, \n                                    \n                                \n\n                            \n                                \n                                    Jian Jian, \n                                    \n                                \n\n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Lizhen Liang, \n                                    \n                                \n\n                            \n                                \n                                    Han Zhuang\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            arXiv preprint arXiv:2203.12800\n                            \n                            \n                            \n                            \n                            \n                            \n                             2022\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n\n                        \n\n                        \n                        \n                            Download PDF\n                        \n                        \n                    \n                \n                        \n        \n        \n    \n\n    \n        \n        \n            \n            \n                 Assigning credit to scientific datasets using article citation networks\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Longfeng Wu, \n                                    \n                                \n\n                            \n                                \n                                    Sarah Bratt, \n                                    \n                                \n\n                            \n                                \n                                    Daniel E Acuna\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            Journal of Informetrics\n                            \n                            \n                             14\n                            \n                            \n                            .2\n                            \n                            \n                             pp. 101013.\n                            \n                            \n                             Elsevier,\n                            \n                            \n                             2020\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n                        \n                            SCIE indexed: SJR Q1\n                        \n                        \n                        \n                        \n\n                        \n\n                        \n                        \n                            Download PDF\n                        \n                        \n                    \n                \n                        \n        \n        \n    \n\n    \n        \n        \n            \n            \n                 Modeling citation worthiness by using attention-based bidirectional long short-term memory networks and interpretable models\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Daniel E Acuna\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            Scientometrics\n                            \n                            \n                             124\n                            \n                            \n                            .1\n                            \n                            \n                             pp. 399-428.\n                            \n                            \n                             Springer,\n                            \n                            \n                             2020\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n                        \n                            SCIE indexed: SJR Q1\n                        \n                        \n                        \n                        \n                        \n                            Dataset Link\n                        \n                        \n\n                        \n                        \n                            Code Link\n                        \n                        \n\n                        \n                        \n                            Download PDF\n                        \n                        \n                    \n                \n                        \n        \n        \n    \n\n\n\n\n\n  \n      CONFERENCES\n\n    \n        \n        \n            \n            \n                 Machine learning and artificial intelligence for science of science and computational discovery: Principles, applications, and future opportunities\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    Daniel E Acuna, \n                                    \n                                \n\n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Han Zhuang, \n                                    \n                                \n\n                            \n                                \n                                    Lizhen Liang\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            iConference 2021 workshop\n                            \n                            \n                            \n                            \n                            \n                            \n                             2021\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n                        \n                            Workshop\n                        \n                        \n                        \n                            Top Conference in Info. Sci.\n                        \n                        \n                        \n                        \n\n                        \n\n                        \n                        \n                            Download PDF\n                        \n                        \n                    \n                \n                        \n        \n        \n    \n\n    \n        \n        \n            \n            \n                 GotFunding: A grant recommendation system based on scientific articles\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Daniel E Acuna\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            Proceedings of the ASIS&T annual meeting\n                            \n                            \n                             57\n                            \n                            \n                            \n                             pp. e323.\n                            \n                            \n                             Wiley Online Library,\n                            \n                            \n                             2020\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n                        \n                            Top Conference in Info. Sci.\n                        \n                        \n                        \n                        \n\n                        \n\n                        \n                        \n                            Download PDF\n                        \n                        \n                    \n                \n                        \n        \n        \n    \n\n    \n        \n        \n            \n            \n                 Large-scale author name disambiguation using approximate network structures\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Daniel E Acuna\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            International conference on computational social science\n                            \n                            \n                            \n                            \n                            \n                            \n                             2020\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n\n                        \n\n                        \n                        \n                            Download PDF\n                        \n                        \n                    \n                \n                        \n        \n        \n    \n\n    \n        \n        \n            \n            \n                 Dead science: Most resources linked in biomedical articles disappear in eight years\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Alain Shema, \n                                    \n                                \n\n                            \n                                \n                                    Daniel E Acuna\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            Intl. Conf. On information\n                            \n                            \n                            \n                            \n                             pp. 170-176.\n                            \n                            \n                             Springer,\n                            \n                            \n                             2019\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n                        \n                            EI Indexed\n                        \n                        \n                        \n                            Top Conference in Info. Sci.\n                        \n                        \n                        \n                        \n\n                        \n\n                        \n                        \n                            Download PDF\n                        \n                        \n                    \n                \n                        \n        \n        \n    \n\n\n\n\n\n  \n      BOOK CHAPTERS\n\n    \n        \n        \n            \n            \n                 Finding datasets in publications: The syracuse university approach\n            \n            \n                \n                    \n                                      \n                            \n                                \n                                    \n                                        Tong Zeng                              \n                                                                    \n                                    \n                                    \n                                        ,\n                                                        \n                                \n\n                            \n                                \n                                    Daniel E Acuna\n                                    \n                                \n\n                            \n                        \n                        \n                            \n                            Rich search and discovery for research datasets: Building the next generation of scholarly infrastructure\n                            \n                            \n                            \n                            \n                             pp. 157-165.\n                            \n                            \n                             SAGE Publications,\n                            \n                            \n                             2020\n                            \n                        \n                    \n                \n                \n                    \n                        \n                        \n                        \n                            Book Chapter\n                        \n                        \n                        \n                        \n\n                        \n\n                        \n                        \n                            Download PDF\n                        \n                        \n                    \n                \n                        \n        \n        \n    \n\n\n\n\n\nNo matching items\n\n\n    \n  \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "publications/predicting-the-longevity-of-resources/index.html",
    "href": "publications/predicting-the-longevity-of-resources/index.html",
    "title": "Predicting the longevity of resources shared in scientific publications",
    "section": "",
    "text": "Download Preprint PDF\n\n\n\n\n Back to topCitationBibTeX citation:@article{e_acunaz2022,\n  author = {E Acunaz, Daniel and Jian, Jian and Zeng, Tong and Liang,\n    Lizhen and Zhuang, Han},\n  publisher = {arXiv.org},\n  title = {Predicting the Longevity of Resources Shared in Scientific\n    Publications},\n  journal = {arXiv preprint arxiv.2203.12800},\n  date = {},\n  url = {https://arxiv.org/abs/2203.12800},\n  doi = {https://doi.org/10.48550/arxiv.2203.12800},\n  langid = {en},\n  abstract = {Research has shown that most resources shared in articles\n    (e.g., URLs to code or data) are not kept up to date and mostly\n    disappear from the web after some years (Zeng et al., 2019). Little\n    is known about the factors that differentiate and predict the\n    longevity of these resources. This article explores a range of\n    explanatory features related to the publication venue, authors,\n    references, and where the resource is shared. We analyze an\n    extensive repository of publications and, through web archival\n    services, reconstruct how they looked at different time points. We\n    discover that the most important factors are related to where and\n    how the resource is shared, and surprisingly little is explained by\n    the author’s reputation or prestige of the journal. By examining the\n    places where long-lasting resources are shared, we suggest that it\n    is critical to disseminate and create standards with modern\n    technologies. Finally, we discuss implications for reproducibility\n    and recognizing scientific datasets as first-class citizens.}\n}\nFor attribution, please cite this work as:\nE Acunaz, Daniel, Jian Jian, Tong Zeng, Lizhen Liang, and Han Zhuang.\nn.d. “Predicting the Longevity of Resources Shared in Scientific\nPublications .” arXiv Preprint Arxiv.2203.12800.\nhttps://doi.org/https://doi.org/10.48550/arxiv.2203.12800."
  },
  {
    "objectID": "publications/large-scale-author-name-disambiguation/index.html",
    "href": "publications/large-scale-author-name-disambiguation/index.html",
    "title": "Large-scale author name disambiguation using approximate network structures",
    "section": "",
    "text": "Download Preprint PDF\n\n    Visit Video Presentation"
  },
  {
    "objectID": "publications/large-scale-author-name-disambiguation/index.html#introduction",
    "href": "publications/large-scale-author-name-disambiguation/index.html#introduction",
    "title": "Large-scale author name disambiguation using approximate network structures",
    "section": "1 Introduction",
    "text": "1 Introduction\nProperly identifying the author of a scientific article is an important task for giving credit, tracking progress, and identifying ideas’ lineages. Usually, publications and citations do not provide unique identifiers to authors but only the raw string character representation of their name and affiliation. The fundamental problem is that an author might change the string representations due to changing in name spelling (e.g., removing accents), journal limitations (e.g., only allow first letter of first name), or simply two people having the same name. Several researchers have proposed methods to solve this problem[1,2,3], but most methods do not scale well and are not open to the community. In this work, we develop a scalable method that we make publicly available to disambiguate large-scale publications."
  },
  {
    "objectID": "publications/large-scale-author-name-disambiguation/index.html#methods",
    "href": "publications/large-scale-author-name-disambiguation/index.html#methods",
    "title": "Large-scale author name disambiguation using approximate network structures",
    "section": "2 Methods",
    "text": "2 Methods\nWe proposed a method named ANNGC (Approximate Nearest Neighbors Graph Clustering), which contains three components: blocking, linkage, and clustering.\n\n2.1 Blocking\nThe aim of blocking is to roughly partition S into a number of groups called blocks B = {b_1 b_2, ... b_k } and then perform disambiguation only within a block. By blocking, the computational complexity of a typical clustering algorithm O(|S|^2) is reduced to O(\\sum{_{i=0}^{|B|}}|b_{i}|^{2}). By choosing |b_i|&lt;&lt;|S|, the difference in complexity is substantial.\n\n\n2.2 Linkage\nTraining a function predicts the probability of two signatures belonging to the same author. In order to convert the signature pairs into feature vectors, we convert co-author names and abstract into vectors using word-wise tfidf, for the first name, full name, affiliation, title, and journal we use character-wise tfidf. We then compute the cosine similarity between those tfidf vectors of the signature pairs. We also use absolute year differences as features. All the features are fed into a logistic regression classifier.\n\n\n2.3 Clustering\nSeveral previous studies have used Hierarchical Agglomerative Clustering (HAC) to partition signatures into groups. . However, as the time complexity of HAC is O(n^3), it is difficult to scale to large blocks. Also, each merging step in HAC requires global distance information of the whole block, thus difficult to distribute the model into multiple nodes. In order to overcome these problems, we propose a scalable and distributable algorithm whose steps are described now (see Figure 1).\n\n\n\n\n\n\nFigure 1: Main steps of ANNGC algorithm\n\n\n\nPlease read the paper for explanation of each step.\n\n\n2.4 Results\nBaseline model: The HAC for clustering is used as a baseline. The blocking and linkage function are the same as ANNGC. As shown in Table 1, ANNGC local cut has the best precision, but the overall performance is slightly lower than the HAC. The local cut is better than global cut in both methods, as the thresholds are customized for each block.\n\n\n\nTable 1: Performance on test set\n\n\n\n\n\nDescription\nPrecision\nRecall\nF1\n\n\n\n\nBaseline-Global cut\n0.978\n0.9793\n0.9788\n\n\nBaseline-local cut\n0.997\n0.9946\n0.9962\n\n\nANNGC-Global cut\n0.968\n0.9766\n0.9727\n\n\nANNGC-local cut\n0.998\n0.9523\n0.9747\n\n\n\n\n\n\n\n\n2.5 Conclusion\nWhile the performance of ANNGC is slightly lower than HAC, our approach has three advantages: 1) less time complexity. The complexity is O(V·h·h_root(V)) for ANN plus O(E·logV) for MST, much lower than HAC (O(V^3)), given V the number of vertices, E the number of edges. 2) distributable, all the steps (ANN, MST, connected component) could be implemented without having the information of the whole blocks. 3) fewer hyper-parameters. There are only two parameters, the k for nearest neighbors and the cut-off threshold. These features are critical for large scale author name disambiguation which is not capable for HAC. In the future, we will experiment with network embeddings which could reduce our vector dimension from million to thousands. Also, we could use more information about the paper, such as the reference list. We could also apply more advanced models for the linkage function."
  },
  {
    "objectID": "publications/finding-datasets-in-publications/index.html",
    "href": "publications/finding-datasets-in-publications/index.html",
    "title": "Finding datasets in publications: the Syracuse University approach",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "publications/finding-datasets-in-publications/index.html#introduction",
    "href": "publications/finding-datasets-in-publications/index.html#introduction",
    "title": "Finding datasets in publications: the Syracuse University approach",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe problem of dataset extraction has been explored before. Ghavimi et al. (2016,2017) use a relatively simple TF-IDF representation with cosine similarity for matching dataset identification in social science articles. Their method consists of three major steps: preparing a curated dictionary of typical mention phrases, detecting dataset references, and ranking matching datasets based on cosine similarity of TF-IDF representations. This approach achieved a relatively high performance, with F1 = 0.84 for mention detection and F1 = 0.83, for matching. Singhal and Srivastava (2013) proposed a method using normalized Google distance to screen whether a term is in a dataset. However, this method relies on external services and is not computationally efficient. They achieve a good F1 = 0.85 using Google search and F1 = 0.75 using Bing. A somewhat similar project was proposed by Lu et al. (2012). They built a dataset search engine by solving the two challenges: identification of the dataset and association to a URL. They build a dataset of 1000 documents with their URLs, containing 8922 words or abbreviations representing datasets. They also build a web-based interface. This shows the importance of dataset mention extraction and how several groups have tried to tackle the problem.\nIn this chapter, we describe a method for extracting dataset mentions based on a deep recurrent neural network. In particular, we used a bidirectional long short-term memory (bi-LSTM) sequence to sequence model paired with a conditional random field (CRF) inference mechanism. The architecture is similar to that of Chapter 6, but we only focus on the detection of dataset mentions. We tested our model on a novel dataset produced for the Rich Context Competition challenge. We achieved a relatively good performance of F1 = 0.885. We discuss the limitations of our model."
  },
  {
    "objectID": "publications/finding-datasets-in-publications/index.html#the-proposed-method",
    "href": "publications/finding-datasets-in-publications/index.html#the-proposed-method",
    "title": "Finding datasets in publications: the Syracuse University approach",
    "section": "2 The Proposed Method",
    "text": "2 The Proposed Method\n\n2.1 Overall View of the Architecture\nIn this section we propose a model for detecting mentions based on a bi-LSTM CRF architecture. At a high level, the model uses a sequence-to-sequence recurrent neural network that produces the probability of whether a token belongs to a dataset mention. The CRF layer takes those probabilities and estimates the most likely sequence based on constraints between label transitions (e.g., mention–to–no-mention–to–mention has low probability). While this is a standard architecture for modelling sequence labelling, the application to our particular dataset and problem is new.\nWe now describe in more detail the choices of word representation, hyperparameters and training parameters. A schematic view of the model is given in Figure Figure 1 and the components are as follow.\n\n\n\n\n\n\nFigure 1: The architecture of bi-LSTM CRF network\n\n\n\n\nCharacter encoder layer: treat a token as a sequence of characters and encode the characters by using a bi-LSTM to get a vector representation.\nWord embedding layer: mapping each token into fixed-size vector representation by using a pre-trained word vector.\nBi-LSTM layer: make use of bi-LSTM network to capture the high-level representation of the whole token sequence input.\nDense layer: project the output of the previous layer to a low-dimensional vector representation of the distribution of labels.\nCRF layer: find the most likely sequence of labels.\n\nPlease read the paper for more details."
  },
  {
    "objectID": "publications/finding-datasets-in-publications/index.html#results",
    "href": "publications/finding-datasets-in-publications/index.html#results",
    "title": "Finding datasets in publications: the Syracuse University approach",
    "section": "3 Results",
    "text": "3 Results\nIn this work we wanted to propose a model for the Rich Context Competition challenge. We propose a relatively standard architecture based on the bi-LSTM CRF network. We now describe the evaluation metrics, hyperparameter setting, and the results of this network on the dataset provided by the competition.\nFor all of our results, we use F1 as the measure of performance. This measure is the harmonic average of the precision and recall and it is the standard measure used in sequence labelling tasks. It varies from 0 to 1, the higher the better. Our method achieved a relatively high F1 of 0.885 for detecting mentions.\nPlease read the paper for more details."
  },
  {
    "objectID": "publications/finding-datasets-in-publications/index.html#conclusion",
    "href": "publications/finding-datasets-in-publications/index.html#conclusion",
    "title": "Finding datasets in publications: the Syracuse University approach",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nIn this work, we report a high-accuracy model for the problem of detecting dataset mentions. Because our method is based on a standard bi-LSTM CRF architecture, we expect that updating our model with recent developments in neural networks would only benefit our results. We also provide some evidence of how difficult we believe the linkage step of the challenge could be if dataset noise is not lowered.\nOne of the shortcomings of our approach is that the architecture is lacking some modern features of RNN networks. In particular, recent work has shown that attention mechanisms are important especially when the task requires spatially distant information, as in this case. These benefits could also translate to better linkage. We are exploring new architectures using self-attention and multiple-head attention. We hope to share these approaches in the near future.\nThere are a number of improvements that we could make in the future. A first improvement would be to use non-recurrent neural architectures such as the Transformer which has been shown to be faster and a more effective learner than RNNs. Another improvement would be to bootstrap information from other dataset sources such as open-access full-text articles from PubMed Open Access Subset. This dataset contains dataset citations (Zeng et al., 2020) – in contrast to the most common types of citations to publications. The location of such citations within the full text could be exploited to perform entity recognition. While this would be a somewhat different problem than the one solved in this chapter, it would still be useful for the goal of tracking dataset usage. In sum, by improving the learning techniques and the dataset size and quality, we could significantly increase the success of finding datasets in publications.\nOur proposal, however, is surprisingly effective. Because we have barely modified a general RNN architecture, we expect that our results will generalize relatively well either to the second phase of the challenge or even to other disciplines. We would emphasize, however, that the quality of the dataset has a great deal of room for improvement. Given how important this task is for the whole of science, we should strive to improve the quality of these datasets so that techniques like this one can be more broadly applied."
  },
  {
    "objectID": "publications/assigning-credit-sci-dataset/index.html",
    "href": "publications/assigning-credit-sci-dataset/index.html",
    "title": "Assigning credit to scientific datasets using article citation networks",
    "section": "",
    "text": "Download Preprint PDF\n    Visit Published DOI"
  },
  {
    "objectID": "publications/assigning-credit-sci-dataset/index.html#introduction",
    "href": "publications/assigning-credit-sci-dataset/index.html#introduction",
    "title": "Assigning credit to scientific datasets using article citation networks",
    "section": "1 Introduction",
    "text": "1 Introduction\nA citation network is an important source of analysis in science. Citations serve multiple purposes such as crediting an idea, signaling knowledge of the literature, or critiquing others’ work (Martyn, 1975). When citations are thought of as impact, they inform tenure, promotion, and hiring decisions (Meho & Yang, 2007). Furthermore, scientists themselves make decisions based on citations, such as which papers to read and which articles to cite. Citation practices and infrastructures are well-developed for journal articles and conference proceedings. However, there is much less development for dataset citation. This gap affects the increasingly important role that datasets play in scientiﬁc reproducibility (Belter, 2014; On Data Citation Standards & Practices, 2013; Park, You, & Wolfram, 2018; Robinson-García, Jiménez-Contreras, & Torres-Salinas, 2016), where studies use them to conﬁrm or extend the results of other research (Darby et al., 2012; Sieber & Trumbo, 1995). One historical cause of this gap is the difﬁculty in archiving datasets. While less problematic today, the citation practices for datasets take time to develop. Better algorithmic approaches to track dataset usage could improve this state. In this work, we hypothesize that a network ﬂow algorithm could track usage more effectively if it propagates publication and dataset citations differently. With the implementation of this algorithm, then, it will be possible to correct differences in citation behavior between these two types of artifacts, increasing the importance of datasets as ﬁrst class citizens of science.\nIn this article, we develop a method for assigning credit to datasets from citation networks of publications, assuming that dataset citations have biases. Importantly, our method does not modify the source data for the algorithms. The method does not rely on scientists explicitly citing datasets but infers their usage. We adapt the network ﬂow algorithm of Walker, Xie, Yan, and Maslov (2007) by including two types of nodes: datasets and publications. Our method simulates a random walker that takes into account the differences between obsolescence rates of publications and datasets, and estimates the score of each dataset—the DataRank. We use the metadata from the National Center for Bioinformatics (NCBI) GenBank nucleic acid sequence and Figshare datasets to validate our method. We estimate the relative rank of the datasets with the DataRank algorithm and cross-validate it by predicting the actual usage of them—number of visits to the NCBI dataset web pages and downloads of Figshare datasets. We show that our method is better at predicting both types of usages compared to citations and has other qualitative advantages compared to alternatives. We discuss interpretations of our results and implications for data citation infrastructures and future work."
  },
  {
    "objectID": "publications/assigning-credit-sci-dataset/index.html#why-measure-dataset-impact",
    "href": "publications/assigning-credit-sci-dataset/index.html#why-measure-dataset-impact",
    "title": "Assigning credit to scientific datasets using article citation networks",
    "section": "2 Why measure dataset impact?",
    "text": "2 Why measure dataset impact?\nScientists may be incentivized to adopt better and broader data sharing behaviors if they, their peers, and institutions are able to measure the impact of datasets (e.g., see Kidwell et al., 2016; Silvello, 2018). In this context, we review impact assess- ment conceptual frameworks and studies of usage statistics and crediting of scientiﬁc works more speciﬁcally. These areas of study aim to develop methods for scientiﬁc indicators of the usage and impact of scholarly outputs. Impact assessment research also derives empirical insights from research products by assessing the dynamics and structures of connections between the outputs. These connections can inform better policy-making for research data management, cyberinfrastructure implementation, and funding allocation.\nMethods for measuring usage and impact include a variety of different dimensions of impact, from social media to code use and institutional metrics. Several of these approaches recognize the artiﬁcial distinction between the scientiﬁc process and product (Priem, 2013). For example, altmetrics is one way to measure engagement with diverse research products and to estimate the impact of non-traditional outputs (Priem, 2014). Researchers predict that it will soon become a part of the citation infrastructure to routinely track and value “citations to an online lab notebook, contributions to a software library, bookmarks to datasets from content-sharing sites such as Pinterest and Delicious” (from Priem, 2014). In short, if science has made a difference, it will show up in a multiplicity of places. As such, a correspondingly wider range of metrics are needed to attribute credit to the many locations where research works reﬂect their value. For example, datasets contribute to thousands of papers in NCBI’s Gene Expression Omnibus and these attributions will continue to accumulate, just like paper accumulate citations, for a number of years after the datasets are publicly released (Piwowar, 2013; Piwowar, Vision, & Whitlock, 2011). Efforts to track these other sources of impact include ImpactStory, statistics from FigShare, and Altmetric.com (Robinson-Garcia, Mongeon, Jeng, & Costas, 2017).\nCredit attribution efforts include those by federal agencies to expand the deﬁnition of scientiﬁc works that are not pub- lications. For example, in 2013 the National Science Foundation (NSF) recognized the importance of measuring scientiﬁc artifacts other than publications by asking researchers for “products” rather than just publications. This represents a signiﬁ- cant change in how scientists are evaluated (Piwowar, 2013). Datasets, software, and other non-traditional scientiﬁc works are now considered by the NSF as legitimate contributions to the publication record. Furthermore, real-time science is pre- sented in several online mediums; algorithms ﬁlter, rank, and disseminate scholarship as it happens. In sum, the traditional journal article is increasingly being complemented by other scientiﬁc products (Priem, 2013).\nYet despite the crucial role of data in scientiﬁc discovery and innovation, datasets do not get enough credit (Silvello, 2018). If credit was properly accrued, researchers and funding agencies would use this credit to track and justify work and funding to support datasets—consider the recent Rich Context Competition which aimed at to ﬁlling this gap by detecting dataset mentions in full-text papers (Zeng & Acuna, 2020). Because these dataset mentions are not tracked by current citation networks, this leads to biases in dataset citations (Robinson-García et al., 2016). The FAIR (ﬁndable, accessible, interoperable, reproducible) principles of open research data are one major initiative that is spearheading better practices with tracking digital assets such as datasets (Wilkinson et al., 2016). However, the initiative is theoretical, and lacks technical implementation for data usage and impact assessment. There remains a need to establish methods to better estimate dataset usage."
  },
  {
    "objectID": "publications/assigning-credit-sci-dataset/index.html#materials-and-methods",
    "href": "publications/assigning-credit-sci-dataset/index.html#materials-and-methods",
    "title": "Assigning credit to scientific datasets using article citation networks",
    "section": "3 Materials and methods",
    "text": "3 Materials and methods\n\n3.1 Datasets\n\n3.1.1 OpenCitations Index (COCI)\nThe OpenCitations index (COCI) is an index of Crossref’s open DOI-to-DOI citation data. We obtained a snapshot of COCI (November 2018 version), which contains approximately 450 million DOI-to-DOI citations. Speciﬁcally, COCI contains information including citing DOI, cited DOI, the publication date of citing DOI. The algorithm we proposed in the paper requires the publication year. However, not all the DOIs in COCI have a publication date. We will introduce Microsoft Academic Graph to ﬁll this gap.\n\n\n3.1.2 Microsoft Academic Graph (MAG)\nThe Microsoft Academic Graph is a large heterogeneous graph consisting of six types of entities: paper, author, institution, venue, event, and ﬁeld of study (Sinha et al., 2015). Concretely, the description of a paper consists of DOI, title, abstract, published year, among other ﬁelds. We downloaded a copy of MAG in November 2019, which contains 208,915,369 papers. As a supplement to COCI, we extract the DOI and published year from MAG to extend those DOIs in COCI without a publication date.\n\n\n3.1.3 PMC Open Access Subset (PMC-OAS)\nThe PubMed Central Open Access Subset is a collection of full-text open access journal articles in biomedical and life sciences. We obtained a snapshot of PMC-OAS in August 2019. It consists of about 2.5 million full-text articles organized in well-structured XML ﬁles. The articles are identiﬁed by a unique id called PMID. We also obtained a mapping between PMIDs and DOIs from NCBI, which enabled us to integrate PMC-OAS into the citation network.\n\n\n3.1.4 GenBank\nGenBank is a genetic sequence database that contains an annotated collection of all publicly available nucleotide sequences for almost 420,000 formally described species (Sayers et al., 2019). We obtained a snapshot of the GenBank database (version 230) with 212,260,377 gene sequences. We remove those sequences without submission date. This left us with 77,149,105 sequences.\n\n\n3.1.5 Figshare\nFigshare is a multidisciplinary, open access research repository where researchers can deposit and share their research output. Figshare allows users to upload various formats of research output, including ﬁgures, datasets, and other media (Thelwall & Kousha, 2016). In order to encourage data sharing, all research data made publicly available has unlimited storage space and is allocated a DOI. This DOI is used by scientists to cite Figshare resources using traditional citation methods. Figshare makes the research data publicly and permanently available which mitigates the resource decay problem and improves research reproducibility (Zeng, Shema, & Acuna, 2019). A Figshare DOI contains the string ‘ﬁgshare’ (e.g., ‘10.6084/m9.ﬁgshare.6741260’). We can leverage this feature to determine whether a publication cites Figshare resources by detecting Figshare-generated DOIs.\n\n\n\n3.2 Construction of citation network\nThe citation networks in this paper consist of two types of nodes and two types of edges. The node is represented by the paper and the dataset and the edge is represented by the citation between two nodes. Concretely, papers cite each other to form paper-paper edges as datasets can only be cited by papers which are represented by the paper-dataset edges. As shown in the construction workﬂow (Fig. 2), we build the paper-paper citation network using COCI and MAG and build two separate paper-dataset edge sets using GenBank and Figshare. Then we integrate the paper-dataset edge sets into the paper-paper citation network to form two complete citation networks. The construction workﬂow is illustrated in Fig. 2.\nPlease read the full paper for details of data processing.\n\n\n3.3 Visualization of citation network\n\n\n\n\n\n\nFigure 1: Visualization of publication-dataset citation network. Papers appear as red nodes and Datasets are green. The lighter the color, the older the resource. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)\n\n\n\n\n\n3.4 Network models for scientiﬁc artifacts\n\n3.4.1 NetworkFlow\nWe adapt the network model proposed in Walker et al. (2007). This method, which we call NetworkFlow here, is inspired by PageRank and addresses the issue that citation networks are always directed back in time. In this model, each vertex in the graph is a publication. The goal of this method is to propagate a vertex’s impact through its citations.\n\n\n3.4.2 DataRank\nIn this article, we extend NetworkFlow to accommodate different kinds of nodes. The extension considers that the probability of starting at any single node should depend on whether the node is a publication or a dataset. This is, publications and datasets may vary in their relevance period. We call this new algorithm DataRank. Mathematically, we redeﬁne the starting probability in Equation 1 of the ith node as:\n\n\\begin{equation}\n\\rho_{i}^{\\text{DataRank}}=\\begin{cases}\n\\exp\\left(-\\frac{\\text{age}_{i}}{\\tau_{\\text{pub}}}\\right) & \\text{if } i \\text{ is a publication}\\\\\n\\exp\\left(-\\frac{\\text{age}_{i}}{\\tau_{\\text{dataset}}}\\right) & \\text{if } i \\text{ is a dataset}\n\\end{cases}\n\\end{equation}\n\\tag{1}\n\n\n3.4.3 DataRank-FB\nIn DataRank, each time the walker moves, there are two options: to stop with a probability \\alpha or to continue the search through the reference list with a probability 1-\\alpha. However, there may exist a third choice: to continue the search through papers who cite the current paper. In other words, the researcher can move in two directions: forwards and backwards. We call this modified method DataRank-FB. In this method, one may stop with a probability \\alpha-\\beta, continue the search forward with a probability 1-\\alpha, and backward with a probability \\beta. To keep them within the unity simplex, the parameters must satisfy \\alpha&gt;0, \\beta&gt;0, and \\alpha&gt;\\beta,\nThen, we deﬁne another transition matrix from the citation network as follows \nM_{ij}=\\begin{cases}\n\\frac{1}{k_{j}^{\\text{in}}} & \\text{if } j  \\text{ cited by } i \\\\\n0 & \\text{o.w.}\n\\end{cases}\n\\tag{2}\nwhere k_{j}^{\\text{in}} is the number of papers that cite j.\nWe update the average path length to all papers in the network starting from \\rho as\n\n\\begin{equation}\nT=I\\cdot\\rho+(1-\\alpha)W\\cdot\\rho+\\beta M\\cdot\\rho+(1-\\alpha)^{2}W^{2}\\rho+\\beta^{2}M^{2}\\rho+\\cdots\n\\end{equation}\n\\tag{3}"
  },
  {
    "objectID": "publications/assigning-credit-sci-dataset/index.html#results",
    "href": "publications/assigning-credit-sci-dataset/index.html#results",
    "title": "Assigning credit to scientific datasets using article citation networks",
    "section": "4 Results",
    "text": "4 Results\nWe aim at ﬁnding whether the estimation of the rank of a dataset based on citation data is related to a real-world measure of relevance such as page views or downloads. We propose a method for estimating rankings that we call DataRank, which considers differences in citation dynamics for publications and datasets. We also propose some variants to this method, and compare all of them to standard ranking algorithms. We use the data of visits of GenBank datasets and downloads of Figshare datasets as measure of real usage. Thus, we will investigate which of the methods work best for ranking them.\nPlease read the full paper for details."
  },
  {
    "objectID": "publications/assigning-credit-sci-dataset/index.html#discussion",
    "href": "publications/assigning-credit-sci-dataset/index.html#discussion",
    "title": "Assigning credit to scientific datasets using article citation networks",
    "section": "5 Discussion",
    "text": "5 Discussion\nThe goal of this article is to better evaluate the importance of datasets through article citation network analysis. Compared with the mature citation mechanisms of articles, referencing datasets is still in its infancy. Acknowledging the long time the practice of citing datasets will take to be adopted, our research aims at recovering the true importance of datasets even if their citations are biased compared to publications.\nScholars disagree on how to give credit to research outcomes. Regardless of how disputed citations are as a measure of credit, they complement other measures that are harder to quantify such as peer review assessment or real usage such as downloads. Citations, however, are rarely used for datasets (Altman & Crosas, 2013), giving these important research outcomes less credit that they might deserve. Our proposal aims at solving some of these issues by constructing a network ﬂow that is able to successfully predict real usage better than other methods. While citations are not a perfect measure of credit, having a method that can at least attempt to predict usage is advantageous.\nPrevious research has examined ways of normalizing citations by time, ﬁeld, and quality. This relates to our DataRank algorithm in that we are trying to normalize the citations by year and artifact type.\nWe ﬁnd it useful to interpret the effect of different decay times on the performance of DataRank."
  },
  {
    "objectID": "publications/assigning-credit-sci-dataset/index.html#conclusion",
    "href": "publications/assigning-credit-sci-dataset/index.html#conclusion",
    "title": "Assigning credit to scientific datasets using article citation networks",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nUnderstanding how datasets are used is an important topic in science. Datasets are becoming crucial for the reproduction of results and the acceleration of scientiﬁc discoveries. Scientists, however, tend to dismiss citing datasets and therefore there is no proper measurement of how impactful datasets are. Our method uses the publication-publication citation network to propagate the impact to the publication-dataset citation network. Using two databases of real dataset networks, we demonstrate how our method is able to predict actual usage more accurately than other methods. Our results suggest that datasets have different citation dynamics to those of publications. In sum, our study provides a prescriptive model to understand how citations interact in publication and dataset citation networks and gives a concrete method for producing ranks that are predictive of actual usage.\nOur study advances an investigation of datasets and publications with novel ideas from network analysis. Our work puts together concepts from other popular network ﬂow estimation algorithms such as PageRank (Brin & Page, 1998) and NetworkFlow (Walker et al., 2007). While scientists might potentially take a great amount of time to change their cita- tion behavior, we could use techniques such as the one put forth in this article to accelerate the credit assignment for datasets. Ultimately, the need for tracking datasets will only become more pressing and therefore we must adapt or miss the opportunity to make datasets ﬁrst-class citizens of science."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Greetings! ~ My name is Tong Zeng (曾通 in Chinese). \n\nI’m a Researcher and Machine Learning Engineer specializing in the Science of Science and Applied Natural Language Processing, with a broad scientific background that ranges from economics, information science and computer science, and many years of working experience in industry.\nI received my Ph.D. in Information Science from Nanjing University where I am lucky to be advised by Professor Xinning Su in 2022, shortly after completing of the joint Ph.D. research training advised by Dr. Daniel Acuna at Syracuse University from Fall 2017 to Spring 2021.\nPlease visit my publication page or Google Scholar profile for details of my research.\nI also have many years working experience in industry, such as:\nSoftware Development Engineer in JD.com (NASDAQ: JD, the biggest Internet company by revenue in China), where I led the design and implementation of a distributed server‑side rendering system for generating H5 pages for core channels and tens of thousands of promotional activities, with daily page views in the hundreds of millions and peak queries per second (QPS) in the tens of thousands.\nPrincipal Machine Learning Engineer at Geely Automobile Research Institute (Geely is the owner of brands: Volvo Cars, Lotus, Geometry, Lynk & Co, Zeekr, etc.), as one of the founding members of the AI Algorithm Development Team, deeply involved in talent acquisition, the architecture design and implementation of Geely Smart Voice Assistant.\n\n\n Back to top"
  },
  {
    "objectID": "publications/GotFunding-A-grant-recommendation-system/index.html",
    "href": "publications/GotFunding-A-grant-recommendation-system/index.html",
    "title": "GotFunding: A grant recommendation system based on scientific articles",
    "section": "",
    "text": "Download PDF\n    Visit Published DOI"
  },
  {
    "objectID": "publications/GotFunding-A-grant-recommendation-system/index.html#introduction",
    "href": "publications/GotFunding-A-grant-recommendation-system/index.html#introduction",
    "title": "GotFunding: A grant recommendation system based on scientific articles",
    "section": "1 INTRODUCTION",
    "text": "1 INTRODUCTION\nThe ability of scientists to fund themselves plays an important role in a scientistʼs career, sometimes propelling their productivity (Jacob & Lefgren, 2011). Scientists, thus, spend an enormous amount of time finding the right opportunities, writing proposals, and waiting for funding decisions (Herbert, Barnett, Clarke, & Graves, 2013). Past researchers have estimated that the opportunity costs in searching and preparing a grant might not be worth it (Gross & Bergstrom, 2019). Some solutions to this problem include lowering the criteria for junior faculty (Van den Besselaar & Sandstrom, 2015), awarding grants with a lottery (Gross & Bergstrom, 2019), or instituting peer-funding mechanisms (Bollen, Crandall, Junk, Ding, & Börner, 2014). Here we explore yet another alternative that instead uses machine learning to suggest the best-matching grant for a scientist based on her publications. We show that we can cast the problem as a recommendation system trained on historical grant–publication data.\nThere are competing factors involved in finding the right grant. Scientists need to consider funding agencies (e.g., NSF or NIH), career stages (e.g., junior-oriented or senior/leader-oriented), award amounts (e.g., small NSF grant vs large DARPA grant), funding lengths (e.g., 1-year EAGER NSF grant or 5-year CAREER NSF grant), and call relevance (e.g., a particular program within NSF or institute in NIH) (Li & Marrongelle, 2012). Thousands of grant opportunities might be available at any given time, offering hundreds of millions of dollars combined (Boroush, 2016). These opportunities also have ramifications far beyond the receiptʼs career (Lane, 2009). It is therefore hard to navigate these funding opportunities.\nWhile submitting a grant is time-consuming and has low probability of success (Gross and Bergstrom (2019); Bollen et al. (2014)), these low probabilities might be related to a mismatch between the grant submitted and the agency that receives it (Crow, 2020). Rather than changing the preparation and review process, we could improve the quality of the matching between scientists and opportunities. Recommendation systems are a natural way of improving how scientists find relevant information such as publications (e.g., Achakulvisut, Acuna, Ruangrong, and Kording (2016)). A similar process could be applied to grant recommendation systems. Some systems exist (e.g., Elsevierʼs Mendeley Funding) but they are closed source and difficult to evaluate.\nIn this publication, we propose to use historical data of past publication–grant relationships from NIH. We cast the problem as a learning-to-rank recommendation system and show that it can achieve high performance on validation (NDCG@1 = 0.945). We further explore the factors that maximize the quality of the match and describe potential improvements in the future."
  },
  {
    "objectID": "publications/GotFunding-A-grant-recommendation-system/index.html#datasets",
    "href": "publications/GotFunding-A-grant-recommendation-system/index.html#datasets",
    "title": "GotFunding: A grant recommendation system based on scientific articles",
    "section": "2 Datasets",
    "text": "2 Datasets\n\n2.1 Federal RePORTER\nFederal RePORTER is an open and automated data infrastructure that collects data on federally funded research projects and its outcomes. The federal RePORTER includes 1.15 million projects from 2000 to 2019, and involving 18 agencies. Among all the agencies, NIH accounts for 77.3% of the projects. Thus, we focus only on NIH projects. Each of the NIH projects contains a list of the publications acknowledging the grant.\n\n\n2.2 PubMed\nPubMed is a search engine and publication repository maintained by the United States National Library of Medicine (NLM). It provides access to over 30 million publications in biomedical and health science. We use PubMed to retrieve the publication abstract."
  },
  {
    "objectID": "publications/GotFunding-A-grant-recommendation-system/index.html#experiments-and-result",
    "href": "publications/GotFunding-A-grant-recommendation-system/index.html#experiments-and-result",
    "title": "GotFunding: A grant recommendation system based on scientific articles",
    "section": "3 EXPERIMENTS AND RESULT",
    "text": "3 EXPERIMENTS AND RESULT\nPlease read the paper for details."
  },
  {
    "objectID": "publications/GotFunding-A-grant-recommendation-system/index.html#discussion-and-conclusion",
    "href": "publications/GotFunding-A-grant-recommendation-system/index.html#discussion-and-conclusion",
    "title": "GotFunding: A grant recommendation system based on scientific articles",
    "section": "4 DISCUSSION AND CONCLUSION",
    "text": "4 DISCUSSION AND CONCLUSION\nIn our work, we aim at improving how scientists can find relevant grants based on their research interests. We propose to solve this problem by building a recommendation system that learns from historical publication–grant relationships. Our results show that we can achieve a high performance of NDCG@1 = 0.975. Further, we explorer various factors that affect the matching between a publication and grant.\nOne of the limitations of our work is that we only look at grants that were funded in the past. Funding mechanisms might be changing over time and publication topics might also change over time. This means that there is no guarantee that a correct prediction will actually yield a successful match for a future grant. Recommendation systems however benefit from large amounts of data and unless we are able to interview and ask scientists about their opinion on publication to grant matching, it is hard to build a recommendation system otherwise. Finally, even if our recommendations are off by topic, they can still serve as a narrowing step during the initial stages of searchers.\nAnother limitation of our work is that we are using publications already funded by grants. However, our recommendation is trying to solve the opposite problem whereas a scientist wants to find a publication that can initiate funding. Research is still unclear on whether funding changes the direction of research but even if it does, our recommendation could be useful to discover people that have worked on similar problems. We hope to obtain new data in the future about grants that were not funded because they did not meet the criteria of a certain problem. Thus, with data that is available but potentially harder to obtain, some of these issues could be solved.\nOur recommendation system is one of the first ones that offer scientists the ability to match their research to past grants. We think this research direction will benefit specially those who are starting in their career and might not have the human capital to help in finding relevant funding opportunities."
  },
  {
    "objectID": "publications/machine-learning-and-artificial-intelligence-for-science-of-science-and-computational-discovery/index.html",
    "href": "publications/machine-learning-and-artificial-intelligence-for-science-of-science-and-computational-discovery/index.html",
    "title": "Machine learning and artificial intelligence for science of science and computational discovery: Principles, applications, and future opportunities",
    "section": "",
    "text": "Visit Workshop Website\n\n\n\n\n Back to topCitationBibTeX citation:@inproceedings{2021,\n  author = {},\n  title = {Machine Learning and Artificial Intelligence for Science of\n    Science and Computational Discovery: {Principles,} Applications, and\n    Future Opportunities},\n  booktitle = {iConference 2021 workshop},\n  date = {2021-03-17},\n  url = {https://scienceofscience.org/workshops/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Machine Learning and Artificial Intelligence for Science of\nScience and Computational Discovery: Principles, Applications, and\nFuture Opportunities .” 2021. In iConference 2021\nWorkshop. https://scienceofscience.org/workshops/."
  },
  {
    "objectID": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html",
    "href": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html",
    "title": "Dead Science: Most Resources Linked in Biomedical Articles Disappear in Eight Years",
    "section": "",
    "text": "Download PDF\n    Visit Published DOI"
  },
  {
    "objectID": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html#introduction",
    "href": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html#introduction",
    "title": "Dead Science: Most Resources Linked in Biomedical Articles Disappear in Eight Years",
    "section": "1 Introduction",
    "text": "1 Introduction\nReproducibility and replicability are key components of science. Increasingly, this depends on the ability of scientists to use the resources shared in scientiﬁc articles. Many studies have found that resources embedded in scientiﬁc publications suﬀer from decay over time [1–6] directly aﬀecting the incremental nature of science. In particular, biomedical sciences is a discipline that reuses resources regularly (e.g., software [7], protocols [8], and datasets [9]). However, a systematic study of the decay of such resources in biomedical publications is lacking.\nThe mechanisms governing sharing of data and resources are important for science. As early as 2003, the National Institutes of Health published a policy requiring applications for grants greater than $500,000 to include data sharing plans [10]. The National Science Foundation also has policies encouraging data sharing [11]. There are other institutions that recognize the importance of this practice (e.g., [12,13]) and its actual impact on the acceleration of science(e.g., [9]). Sharing of resources is important and how they decay is still poorly understood.\nOne way of understanding how long and why resources are available is to analyze how resources decay over time. Several studies have tried to understand thisphenomenon in several disciplines. All this previous work has focused mostly on closed access publications and, to the best of our knowledge, the biggest dataset has around 1 million URLs [5,6]. In our work, we examine resource decay at a signiﬁcantly larger volume in open access biomedical articles."
  },
  {
    "objectID": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html#materials-and-methods",
    "href": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html#materials-and-methods",
    "title": "Dead Science: Most Resources Linked in Biomedical Articles Disappear in Eight Years",
    "section": "2 Materials and Methods",
    "text": "2 Materials and Methods\nWe obtained a copy of Pubmed Open Access Subset in June 2018 which consists of 1,904,971 articles. Not all URLs in these ﬁles are interesting or represent a resource being shared. We apply the following ﬁlters to discard URLs. First, we remove links to local ﬁle systems, URLs without any paths, and we canonicalize the URLs. The URL availability checker followed standard detection methods [17]. This checker, however, does not consider resources that are available but moved from the original URL. The ﬁnal dataset contains 2,642,694 URLs of which 1,883,622 are unique."
  },
  {
    "objectID": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html#results",
    "href": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html#results",
    "title": "Dead Science: Most Resources Linked in Biomedical Articles Disappear in Eight Years",
    "section": "3 Results",
    "text": "3 Results\n\n3.1 Exponential Growth in Link Sharing.\nWe wanted to examine how resource sharing in the form of URLs has evolved over time. For each year of publication, we computed the number of links per article. This trend is exponential (Figure 1 (a)). We also analyze the percentage of articles with at least one URL. We found that this trend is exponential and that articles published today are more likely than not to have a link to a resource (Figure 1 (b)).\n\n\n\n\n\n\n\n\n\n\n\n(a) Average links per paper as a function publication year. A locally weighted regression (loess, red line) is shown as well.\n\n\n\n\n\n\n\n\n\n\n\n(b) Percent of articles with a link as a function of publication year. A locally weighted regression (“loess”, red line) is shown as well.\n\n\n\n\n\n\n\nFigure 1: Some trends about the resource sharing. There is a clear exponential growth in these trends.\n\n\n\nSee Figure 1 for examples. In particular, Figure 1 (b).\n\n\n3.2 Most Resources Shared in Publications Disappear After Eight Years.\nThe point at which half of the resources become obsolete is important. Here we simply examined the average availability of a resources as a function of age (Figure 2) and found that this point happens after eight years. Surprisingly, we notice that new resources (age = 0) have a 20% chance of being unavailable, similar to previous findings [18]. Resources tend to follow a steady decline from ages 1 to 10 years. Then, it seems that resources 10 years and older stabilize around 42% availability. The data in our research showed that half of links become unavailable after 8 years.\n\n\n\n\n\n\nFigure 2: Probability of resources being available as a function of age in years.\n\n\n\n\n\n3.3 Factors Related to Link Availability.\nThe most positive weights were the number of times a link has been used and the domain suffix “int”. Interestingly, the “org” domain suffix, the size of the resource being shared, and the number of affiliations in the paper had all large positive associations. These positive associations intuitively suggest that links that have been shared in many articles, articles in government or non-profit organizations, and articles with many authors are all factors that contribute to a link being available.\nExpectedly, the most important negative feature is the link’s age. In absolute terms, it is also the biggest contributor to the prediction. The length of the path (related to the length of the URL) is also an important predictor. Domain suffixes related to India (in), the European Union (eu), China (cn), and Korea (kr) are all also negatively related to availability.\n\n\n3.4 Top Cited Links Are Mostly Tools.\nWe performed a qualitative analysis of the most shared links and their availability. Interestingly, most of these highly cited links were tools related to gene expression and sequence search."
  },
  {
    "objectID": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html#discussion-and-conclusion",
    "href": "publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html#discussion-and-conclusion",
    "title": "Dead Science: Most Resources Linked in Biomedical Articles Disappear in Eight Years",
    "section": "4 Discussion and conclusion",
    "text": "4 Discussion and conclusion\nThe practice of embedding links in scientific papers has been growing exponentially, and our findings are in line with previous research [15,3]. While we found a half-life of 8 years, there is great variability in this number—2.2 years [2], 5 years [4], 5.3 years [2], 9.3 years [15], 10 years [14]. However, all this previous work has analyzed a smaller volume of links and shorter time spans compared to our analysis, which may explain this variability.\nWhile our findings are only correlations, they offer some intuitive suggestions. We would propose that authors use shorter, easier to remember URLs, hosted in non-profit domains. For links that are available, we should consider archiving those that are old, have complex URLs, are published in low h-index journals, and are hosted in country-based domains. In none of this is possible, we can explicitly archive links with services such as Perma [16], the Internet Archive [22], and WebCite [23,24]."
  },
  {
    "objectID": "publications/modeling-citation-worthiness/index.html",
    "href": "publications/modeling-citation-worthiness/index.html",
    "title": "Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models",
    "section": "",
    "text": "Download Preprint PDF\n    Visit Published DOI\n    PMOA-CITE dataset\n    Code\n    Online Demo"
  },
  {
    "objectID": "publications/modeling-citation-worthiness/index.html#introduction",
    "href": "publications/modeling-citation-worthiness/index.html#introduction",
    "title": "Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models",
    "section": "1 Introduction",
    "text": "1 Introduction\nScientists and journalists have challenges determining proper citations in the ever increasing sea of information. More fundamentally, when and where a citation is needed—sometimes called citation worthiness—is a crucial first step to solve this challenge. In the general media, some problematic stories have shown that claims need citations to make them verifiable—e.g., the debunked A Rape on Campus article in the Rolling Stone magazine (Wikipedia contributors 2018). Analyses of Wikipedia have revealed that lack of citations correlates with an article’s immaturity (Jack et al. 2014; Chen and Roth 2012). In science, the lack of citations leaves readers wondering how results were built upon previous work (Aksnes and Rip 2009). Also, it precludes researchers from getting appropriate credit, important during hiring and promotion (Gazni and Ghaseminik 2016). The sentences surrounding a citation provide rich information for common semantic analyses, such as information retrieval (Nakov et al. 2004). There should be methods and tools to help scientists cite; in this work, we want to understand where citations should be situated in a paper with the goal of automatically suggesting them.\nRelatively much less work has been done on detecting where a citation should be. He et al. (2011) were the first to introduce the task of identifying candidate location where citations are needed in the context of scientific articles. Jack et al. (2014) studied how to detect citation needs in Wikipedia. Peng et al. (2016) used the learning-to-rank framework to solve citation recommendation in news articles. These are very diverse domains, and therefore it is difficult to generalize results. We contend that a large standard dataset of citation location with open code and services would significantly improve the systematic study of the problem. Thus, the task of citation worthiness detection is relatively new and needs further exploration.\nThe attention mechanism is a relatively recent development in neural networks motivated by human visual attention. Humans get more information from the region they pay attention to, and perceive less from other regions. An attention mechanism in neural networks was first introduced in computer vision (Sun and Fisher 2003), and later applied to NLP for machine translation (Bahdanau et al. 2014). Attention has quickly become adopted in other sub-domains. Luong et al. (2015) examined several attention scoring functions for machine translation. Li et al. (2016) used attention mechanisms to improve results in a question-answering task. Zhou et al. (2016) made use of an attention-based LSTM network to do relational classification. Lin et al. (2017) used attention to improve sentence embedding. Recently, Vaswani et al. (2017) built an architecture called transformer that promises to replace recurrent neural networks (RNNs) altogether by only using attention mechanisms. These results show the advantage of attention for NLP tasks and thus its potential benefit for citation worthiness.\nIn this study, we formulate the detection of sentences that need citations as a classification task that can be effectively solved with a deep learning architecture that relies on an attention mechanism. Our contributions are the following:\n\nA deep learning architecture based on bidirectional LSTM with attention and contextual information for citation worthiness\nA new large scale dataset for the citation worthiness task that is 300 times bigger that the next current alternative\nA set of classic interpretable models that provide insights into the language used for making citations\nAn examination of common citation mistakes—from unintentional omissions to poten- tially problematic mis-citations\nAn evaluation of transfer learning between our proposed dataset and the ACL-ARC dataset\nThe code to produce the dataset and results, a web-based tool for the community to evaluate our predictions, and the pre-processed dataset."
  },
  {
    "objectID": "publications/modeling-citation-worthiness/index.html#data-sources-and-data-preprocessing",
    "href": "publications/modeling-citation-worthiness/index.html#data-sources-and-data-preprocessing",
    "title": "Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models",
    "section": "2 Data sources and data pre‑processing",
    "text": "2 Data sources and data pre‑processing\n\n2.1 PMOA-CITE\nIn this paper, We constructed a new dataset called PMOA-CITE based on PubMed central open access subset.\nPubMed Central Open Access Subset (PMOAS) is a full-text collection of scientific literature in bio-medical and life sciences. PMOAS is created by the US’s National Institutes of Health. We obtain a snapshot of PMOAS on August, 2019. The dataset consists of more than 2 million full-text journal articles organized in well-structured XML files by the National Information Standards Organization (ANSI/NISO 2013).\nWe prepare the PMOA-CITE in the following steps:\n\nSentence segmentation and outlier removal. Text in a PMOAS XML file is marked by a paragraph tag, but there might be other XML tags inside paragraph tags. Therefore, we needed to get all the text of a paragraph from XML tags recursively and break paragraphs into sentences. We used spaCy Python package to do the sentence splitting (Honnibal and Montani 2017). However, there are some outliers in the sentences (e.g., long gene sequences with more then 10 thousand characters that are treated as one sentence). Base on the distribution of sentence length (see Fig. 3), we remove the sentences that are outliers either in character or word length. We winsorize 5% and 95% quantiles. For character-wise length, this amounts to 19 characters for 5% quantile and 275 characters for 95% quantile. For word-wise length, it is 3 words and 42 words, respectively.\nHierarchical tree-like structure. By using section and paragraph tagging information in the XML file and the sentences we extracted in previous step, we construct a hierarchical tree-like structure of the articles. In this structure, sentences are contained within paragraphs, which in turn are contained within sections. For each section, we extract the section-type attribute from the XML file which indicates which kind of section is (from a pre-defined set). For those sections without a section-type, we use the section title instead.\nCitation hints removal. The citing sentence usually has some explicit hints which discloses a citation. This provides too much information for the model training and it does not faithfully represents a real-world application scenario. Thus, we removed all the citation hints by regular expression (see Table 1).\n\n\n\n\nTable 1: Regular expression to remove the citation hints\n\n\n\n\n\n\n\n\n\n\nRegular expression\nDescription\nExample\n\n\n\n\n(?&lt;!^)([\\[\\(])[\\s]* ([\\d][\\s\\,\\-\\–\\;\\-]*)* [\\d][\\s]*[\\]\\)]\nnumbers contained in parentheses and square rackets\n“[1, 2]”, “[ 1- 2]”, “(1-3)”, “(1,2,3)”, “[1-3, 5]”, “[8],[9],[12]”, “( 1-2; 4-6; 8 )”\n\n\n[\\(\\[]\\s*([^\\(\\)\\[\\]]* (((16|17|18|19|20) \\d{2}(?!\\d))| (et[\\. \\s\\\\xa0]*al\\.)) [^\\(\\)]*)?[\\)\\]]\ntext within parentheses\n“(Kim and li, 2008)”, “(Heijman , 2013b)”, “(Tárraga , 2006; Capella-Gutiérrez , 2009)”, “(Kobayashi et al., 2005)”, “(Richart and Barron, 1969; Campion et al, 1986)”, “(Nasiell et al, 1983, 1986)”\n\n\net[\\. \\s\\\\xa0]+al[\\.\\s\\(\\[]* ((16|17|18|19|20)\\d {2})*[)\\] \\s]*(?=\\D)\nremove et al. and the following years\n“et al.”, “et al. 2008”, “et al. (2008)”\n\n\n\n\n\n\n\nNoise removal. We apply the following cleanup steps: trim white spaces at beginning and end of a sentence, remove the numbers or punctuations at the beginning of a sentence, and remove numbers at the end of a sentence.\n\nAfter the processing, we get a dataset (PMOA-CITE) with approximately 309 million sentences. However, due to the computational cost and in order to make all of our analysis manageable, we randomly sample articles whose sentences produce close to one million sentences. We further split the one million sentences, 60% for training, 20% for validating, and 20% for testing."
  },
  {
    "objectID": "publications/modeling-citation-worthiness/index.html#text-representation",
    "href": "publications/modeling-citation-worthiness/index.html#text-representation",
    "title": "Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models",
    "section": "3 Text Representation",
    "text": "3 Text Representation\nSome of our models use different text representations predicting citation worthiness.\n\n3.1 Bag of words (BoW) representation\nWe follow the standard definition of term-frequency inverse term-frequency (tf-idf) to construct our bag of words (BoW) representation (Manning et al. 2008). Our BoW representation for a sentence S which consists of n words will therefore be the vector of all tf-idf values in document D_{i}. \n\\text{BoW}(S)=[\\text{tf-idf}_{w_{1},D_{i}},...,\\text{tf-idf}_{w_{n},D_{i}}]\n\\tag{1}\n\n\n3.2 Topic modeling based (TM) representation\nTopic modeling is a machine learning technique whose goal is to represent a document as a mixture of a small number of “topics”. This reduces the dimensionality needed to represent a document compared to bag-of-words. There are several topic models available including Latent Semantic Analysis (LSA) and Non-negative Matrix Factorization (NMF). In this paper, we use Latent Dirichlet Allocation (LDA), which is one of the most popular and well-motivated approaches.\n\n\n3.3 Distributed word representation\nWhile topic models can extract statistical structure across documents, they do a relatively poor job at extracting information within documents. In particular, topic models are not meant to find contextual relationships between words. Word embedding methods, in contrast, are based on the distributional hypothesis which states that words that occur in the same context are likely to have similar meaning (Harris, 1954). The famous statement “you shall know a word by the company it keeps” by Firth (1957) is a concise guideline for word embedding: a word could be represented by means of the words surrounding it. In word embedding, words are represented as fixed-length vectors that attempt to approximate their semantic meaning within a document.\nThere are several distributed word representation methods but one of the most successful and well-known is GloVe by Pennington et al. (2014). We use GloVe word vectors with 300 dimensions, pre-trained on 6 billion tokens."
  },
  {
    "objectID": "publications/modeling-citation-worthiness/index.html#an-attentionbased-bilstm-architecture-for-citation-worthiness",
    "href": "publications/modeling-citation-worthiness/index.html#an-attentionbased-bilstm-architecture-for-citation-worthiness",
    "title": "Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models",
    "section": "4 An attention‑based BiLSTM architecture for citation worthiness",
    "text": "4 An attention‑based BiLSTM architecture for citation worthiness\nIn this section, we describe our new architecture for improving upon the performance of classic statistical learning models presented above. Importantly, these models might neglect some of the interpretability but might pay large performance dividends. Generally, they do not need hand-crafted features. At a high level, the architecture we propose has the following layers (also Figure 1):\n\nCharacter embedding layer: encode every character in a word using a bidirectional LSTM, and get a vector representation of a word.\nWord embedding layer: convert the tokens into vectors by using pre-trained vectors.\nEncoder layer: use a bidirectional LSTM which captures both the forward and backward information flow.\nAttention layer: make use of an attention mechanism to interpolate the hidden states of the encoder (explained below)\nContextual Features layer: obtain the contextual features by combining features of sec- tion, previous sentence, current sentence, and next sentence.\nClassifier layer: use a multilayer perceptron to produce the final prediction of citation worthiness.\n\n\n\n\n\n\n\nFigure 1: The architecture of the proposed attention-based BiLSTM neural network"
  },
  {
    "objectID": "publications/modeling-citation-worthiness/index.html#results",
    "href": "publications/modeling-citation-worthiness/index.html#results",
    "title": "Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models",
    "section": "5 Results",
    "text": "5 Results\nPlease read the paper for details."
  },
  {
    "objectID": "publications/modeling-citation-worthiness/index.html#discussion",
    "href": "publications/modeling-citation-worthiness/index.html#discussion",
    "title": "Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models",
    "section": "6 Discussion",
    "text": "6 Discussion\nIn this work, we developed methods and a large dataset for improving the detection of citation worthiness. Citation worthiness is an important first step for constructing robust and well-structured arguments in science. It is crucial for determining where sources of ideas should mentioned within a manuscript. Previous research has shown promising results but thanks to our new large dataset and modern deep learning architecture, we were able to achieve significantly good performance. We additionally proposed several techniques to interpret what makes scientists use citations. We uncovered potential issues in citation data and behavior: XML documents not properly tagged, citations in the wrong form, and, even worse, scientists failing to cite when they should have. We make our code and a web-based tool available for the scientific community. Our results and new datasets should contribute to the larger need to complement scientific writing with automated techniques. Taken together, our results suggest that deep learning with modern attention-based mechanisms can be effectively used for citation worthiness. We now describe contributions in the context of other work and potential limitations of our approach.\nAs an enhancement to the ACL-ARC dataset, we proposed the PMOA-CITE dataset in the hope of facilitating research on the citation worthiness task. This extends the datasets available to the field of bio-medical science. Our improvements are 1) a two orders of mag- nitude increase in data size, 2) a well-structured XML file that is less noisy, and 3) contextual information. This dataset could be potentially used in other citation context-related research, such as text summarization (Chen and Zhuge 2019), or citation recommendation (Huang et al. 2015). Therefore, our contribution goes beyond the application of citation worthiness.\nBased on the experiments on PMOA-CITE dataset, the use of contextual features consistently improved the performance. This improvement was independent of the algorithm and text representation used (Tables 4 and 7) . A similar results was reported in He et al. (2010) and Jochim and Schütze (2012)). This suggests that contextual information was key for citation worthiness and other related tasks.\nIn order to facilitate future research, we made our datasets and models available to the public. The links of the dataset and the code parsing XML files are available at https​://githu​b.com/scios​ci/cite-worth​iness​ . We also built a web-based tool (see Fig. 8) at http://cite-worth​iness​.scien​ceofs​cienc​e.org. This tool might help inform journalist, policy makers, the public to better understand the principles of proper source citation and credit assignment."
  },
  {
    "objectID": "blogs/clustering/index.html",
    "href": "blogs/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Image source: https://www.freepik.com",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/clustering/index.html#introduction",
    "href": "blogs/clustering/index.html#introduction",
    "title": "Clustering",
    "section": "Introduction",
    "text": "Introduction\nWhat is clustering? Clustering in machine learning is an unsupervised learning technique that involves grouping a set of data points or objects into subsets, called clusters, based on the inherent patterns or similarities among the data1. For example, if we have a dataset containing information about mall customers, including attributes such as ‘Age,’ ‘Income,’ and ‘Spending Score,’ we input this dataset into the clustering algorithm. The algorithm then generates several clusters, considering the dataset’s characteristics and the parameters of the specific clustering algorithm employed.\n\nUnsupervised Learning\nAs clustering is the a typical type of unsupervised learning alglrithm, it is important to first understand what unsupervised learning.\nUnsupervised learning is the conceptual counterpart of supervised learning. It is a category of machine learning where the algorithm is tasked with finding patterns or structures in input data without explicit guidance or labeled output. Unlike supervised learning, there are no predefined target labels for the algorithm to learn from. The goal is to explore the inherent structure within the data, making it a form of self-discovery.\nIn unsupervised learning, the algorithm seeks to identify relationships, groupings, or representations within the data to uncover hidden patterns or insights. This can involve tasks such as clustering, where similar data points are grouped together, or dimensionality reduction, where the goal is to reduce the number of features while preserving essential information.",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/clustering/index.html#clustering-in-machine-learning",
    "href": "blogs/clustering/index.html#clustering-in-machine-learning",
    "title": "Clustering",
    "section": "Clustering in Machine Learning",
    "text": "Clustering in Machine Learning\nSimilar to unsupervised learning, the primary objective of clustering is to organize the data in such a way that items within the same cluster are more similar to each other than to those in other clusters. Clustering does not rely on predefined labels for the data; instead, it discovers the structure within the data itself. Some of the examples of clustering application are listed below:\n\nCustomer Segmentation: Grouping customers based on their purchasing behavior, preferences, or demographic information. For example,an e-commerce company might use clustering to identify segments of customers with similar buying patterns, allowing for targeted marketing strategies.\nDocument Clustering: Organizing a collection of documents into groups based on their content or topic. For example, news articles on a website can be clustered into topics such as sports, politics, and entertainment.\nAnomaly Detection: Identifying unusual or unexpected patterns in data that do not conform to normal behavior. For example, monitoring network traffic and clustering unusual patterns to detect potential security threats.\nSocial Network Analysis: Identifying communities or groups within a social network based on interactions between users. For example, Clustering users on a social media platform based on their connections and shared interests.\n\nClustering is a versatile technique in machine learning that finds applications across various domains, contributing to data exploration, pattern discovery, and decision-making based on inherent similarities within the data.",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/clustering/index.html#clustering-algorithms",
    "href": "blogs/clustering/index.html#clustering-algorithms",
    "title": "Clustering",
    "section": "Clustering Algorithms",
    "text": "Clustering Algorithms\nAs listed above, clustering could be applied in various applications. Different applications have different data characteristics which might require different clustering algorithms. Typically, the clustering algorithms could be categorized into five types2:\n\nConnectivity-based clustering:\nConnectivity-based clustering focuses on the relationships or connections between data points in the feature space. It often involves methods that identify clusters based on the concept of connectivity which is essentially the distances between data points. One notable connectivity-based clustering algorithm is Hierarchical Agglomerative Clustering (HAC).\nCentroid-based clustering:\nCentroid-based clustering is a type of clustering algorithm that organizes data points into clusters based on the proximity to the centroid of each cluster. The centroid is a representative point that minimizes the sum of squared distances from itself to all points in the cluster. This type of clustering is commonly used in applications where clusters can be well approximated by their central points. One typical centroid-based clustering is K-Means Clustering.\nDistribution-based clustering:\nDistribution-based clustering, also known as model-based clustering, is a type of clustering algorithm that assumes that the data is generated from a mixture of probability distributions. The fundamental idea is that the data points are modeled as being generated from a combination of several underlying probability distributions, and the goal of the algorithm is to identify these distributions and assign data points to the most likely one. One popular algorithm in the category of distribution-based clustering is the Gaussian Mixture Model (GMM).\nDensity-based clustering:\nDensity-based clustering is a type of clustering algorithm that groups data points based on their density in the feature space. Unlike centroid-based clustering, which relies on the notion of central points, density-based clustering identifies dense regions of points and separates them from sparser regions. A prominent example of a density-based clustering algorithm is DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\nGrid-based clustering:\nGrid-based clustering is a type of clustering algorithm that divides the data space into a set of cells, forming a grid structure. The primary idea is to use this grid to efficiently organize and analyze the data points, identifying dense regions or clusters based on the distribution of points within the cells. Grid-based clustering methods are particularly useful when dealing with large datasets or datasets with varying point densities. One well-known example of a grid-based clustering algorithm is STING (STatistical INformation Grid).\n\n\nApplications in Customer Segmentation\nIn the section above, we briefly described five different types of clustering algorithms, but given the space, we can’t discuss them all in details in this post. We plan to use the two most popular algorithms, K-Means as well as DBSCAN to solve a real world problem, namely and as mentioned before, the Customer Segmentation.\nIn this Customer Segmentation task, we will use a data about the customers like the Customer ID, age, gender, income etc, the goal is break down the customers into a few distinct subgroups, so that, we can use the subgroups to understand the customers better and further apply different marketing strategy on different subgroups.",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/clustering/index.html#methods",
    "href": "blogs/clustering/index.html#methods",
    "title": "Clustering",
    "section": "Methods",
    "text": "Methods\nIn this post, we’re going to use two of the most popular clustering algorithms, namely K-Means and DBSCAN for customer segmentation. The description of these two algorithms will be covered in this section.\n\nK-Means\nK-Means is a popular clustering algorithm that partitions a dataset into K distinct, non-overlapping subsets (clusters). Each data point belongs to the cluster with the nearest mean, serving as the prototype of the cluster. The algorithm minimizes the intra-cluster variance, aiming to create cohesive and well-separated clusters.\nThe optimization objective (loss function) introduced above could be expressed as3:\n\n{\\displaystyle J= \\sum_{i=1}^{k}\\sum_{j=1}^{n}\\left\\|\\ {x_j} -c_{i}\\right\\|^{2}}\n\nwhere:\n\nJ is the objective function which minimize the total squared distance of data points to their assigned cluster centroids.\nk is the number of clusters.\nn is the number of data points.\nx_j is a data point.\nc_i is the centroid of cluster i.\n\n\n\nDBSCAN\nDBSCAN is a density-based clustering algorithm that partitions a dataset into clusters based on the density of data points. Unlike K-Means, DBSCAN does not require specifying the number of clusters beforehand and can discover clusters of arbitrary shapes. It is particularly effective at identifying clusters in datasets with varying densities and handling noise.\nDBSCAN optimizes the following loss function4:\n\n\n\n\n\n\nNote\n\n\n\nFor any possible clustering {\\displaystyle C=\\{C_{1},\\ldots ,C_{l}\\}} out of the set of all clusterings {\\displaystyle {\\mathcal {C}}} it minimizes the number of clusters under the condition that every pair of points in a cluster is density-reachable, which corresponds to the original two properties “maximality” and “connectivity” of a cluster:\n\n{\\displaystyle \\min _{C\\subset {\\mathcal {C}},~d_{db}(p,q)\\leq \\varepsilon ~\\forall p,q\\in C_{i}~\\forall C_{i}\\in C}|C|}\n\nwhere {\\displaystyle d_{db}(p,q)} gives the smallest {\\displaystyle \\varepsilon } such that two points p and q are density-connected.\n\nsource: https://en.wikipedia.org/wiki/DBSCAN\n\n\n\nFor both the K-Means5 and DBSCAN6, we will use the implementation in the scikit-learn library.",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/clustering/index.html#data",
    "href": "blogs/clustering/index.html#data",
    "title": "Clustering",
    "section": "Data",
    "text": "Data\nThe data used in this post is the Mall Customers Data hosted on Github7. The dataset is distributed in csv file and consist of 5 columns. The columns name and its meaning are explained as below:\n\nCustomerID: A string represents the identification number of the customer.\nGender: A categorical variable consist of two levels, male or female.\nAge: An integer variable denotes the age of a customer.\nAnnual Income: An integer variable represents the annual income in kilo $.\nSpending Score: An integer variable the customer being assigned based on their behavior and purchasing data.\n\n\nExploratory Data Analysis\nImport the libraries we might use in this blog:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nNow, load the data and rename the columns to get more readable names:\n\ndf = pd.read_csv(\"data/Mall_Customers.csv\").rename(columns={\n    \"Genre\":\"Gender\", \n    \"Annual Income (k$)\":\"AnnualIncome\",\n    \"Spending Score (1-100)\":\"SpendingScore\"\n})\ndf.head()\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnualIncome\nSpendingScore\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n\n\n\n\n\n\nDescriptive Statistics\nLet’s take a look at the descriptive statistics:\n\ndf.describe()\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnualIncome\nSpendingScore\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\n\ndf.isnull().sum()\n\nCustomerID       0\nGender           0\nAge              0\nAnnualIncome     0\nSpendingScore    0\ndtype: int64\n\n\nThere are no missing data, it is good for demonstration purpose since we can more focused on core parts, but please keep in mind that it is unlikely the scenario in real life applications where we put significant amount of time to clean and preprocessing the data before analysis and modeling stage.\n\n\nData Visualization\nThere is one category filed named Gender, it has two levels, male and female. There distribution of these two types are as follows:\n\ngender_level_count = df['Gender'].value_counts()\nlabels = ['Male','Female']\nplt.pie(gender_level_count, labels=labels, autopct='%.0f%%')\nplt.show()\n\n\n\n\n\n\n\n\nThere are slightly little more male than female. Let’s then, analysis the other three numerical fields:\n\nsns.pairplot(df[[\"Gender\", \"Age\", \"AnnualIncome\", \"SpendingScore\"]], hue='Gender', aspect=1.5, height=2.85)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot above separates the data into male and female group and display the male group data distribution in blue and female group in yellow.\nThe diagonal represents the data distribution of the corresponding field, we can infer that:\n\nThe distribution of all these fields is not perfectly normal distributed, but has a bell shape with one peak and descents at both left and right side.\nThere is not significant subgroups if we only looks at one field.\n\nIf we examine each field individually:\n\nAge group near 30-40 has the highest density.\nMost of customers have income 40-90k.\nMost of customers have spending score around 50.\n\nIf we check the subplots in pairs, we can find that there are no significant relationship between the pairs, except for the relationship between Annual Income and Spending Score , the data points are clearly divided into five subgroups.",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/clustering/index.html#experiments",
    "href": "blogs/clustering/index.html#experiments",
    "title": "Clustering",
    "section": "Experiments",
    "text": "Experiments\nWe now start clustering analysis with K-Means algorithm, we will use the sklearn library.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\n\n\nX = df[['Age', 'AnnualIncome', 'SpendingScore']]\nwcss= []\nfor k in range(1, 11):\n    kmeans = KMeans(n_clusters = k, n_init='auto', init = 'k-means++')\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.figure(figsize = (8, 4))\nplt.plot(range(1, 11), wcss, linewidth = 2,  marker='*')\nplt.title('Elbow Plot\\n', fontsize = 20)\nplt.xlabel('K')\nplt.ylabel('WCSS (Within-Cluster Sum of Square)')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the elbow plot, we can infer that K=6 could be the best number of cluster. Let’s then use the number of clusters as 6 to train the K-Means and plot the distribution of predicted clusters using three dimension plot as follows:\n\nimport plotly.express as px\nX = df[['Age', 'AnnualIncome', 'SpendingScore']]\nkmeans = KMeans(n_clusters = 6, n_init='auto')\nclusters = kmeans.fit_predict(X)\nX['label'] = clusters\nfig = px.scatter_3d(X, x=\"AnnualIncome\", y=\"SpendingScore\", z=\"Age\",\n                    color = 'label', size = 'label', width=750, height=600)\nfig.show()\n\n                                                \n\n\nWe are now going to use the DBSCAN to carry out the clustering analysis:\n\nfrom sklearn.cluster import DBSCAN\n\ndb = DBSCAN(eps=11, min_samples=6).fit(X)\nX['label'] = clusters\nfig = px.scatter_3d(X, x=\"AnnualIncome\", y=\"SpendingScore\", z=\"Age\",\n                    color = 'label', size = 'label', width=750, height=600)\nfig.show()\n\n                                                \n\n\n\nFor DBSCAN we don’t need to choose the hyper parameter K anymore, the algorithm will divide the dataset into 5 clusters automatically, which is different than the result of K-Means (6 clusters).",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/clustering/index.html#discussion-and-conclusion",
    "href": "blogs/clustering/index.html#discussion-and-conclusion",
    "title": "Clustering",
    "section": "Discussion and Conclusion",
    "text": "Discussion and Conclusion\nIn this plot, we introduced the clustering algorithm as a classical type of unsupervised learning and applied two clustering techniques, namely K-Means and DBSCAN in the customer segmentation task. Our analysis and visualization shows the steps and differences between these two algorithms when applied in real world problems.\nBoth the K-Means and DBSCAN aim to group similar data points together into clusters based on the data characteristics. They are both unsupervised learning techniques, meaning they don’t rely on labeled training data and instead can discover patterns or structure within the data itself.\nWhile K-Means and DBSCAN share lots of traits in nature, here we wanted to mention some of the key differences between K-Means and DBSCAN:\n\nAlgorithm Type:\nK-Means is a centroid-based clustering algorithm. It aims to partition data into K clusters, where each cluster is represented by its centroid.\nDBSCAN is a density-based clustering algorithm. It groups together data points that are close to each other and have a sufficient number of neighboring points.\nCluster Number:\nK-Means requires the user to specify the number of clusters (K) beforehand, and it assigns each data point to the nearest centroid.\nDBSCAN does not require the user to specify the number of clusters. It automatically discovers clusters based on the density of data points.\nHandling Noisy Data:\nK-Means sensitive to outliers and noise because it tries to assign all data points to a cluster, even if they do not belong to any clear cluster.\nDBSCAN can identify and label outliers as noise, making it more robust to outliers and better at handling data with varying densities.\nParameter Sensitivity:\nK-Means is sensitive to the initial placement of centroids, and the final result may depend on the initial cluster centers.\nDBSCAN is less sensitive to the choice of parameters, such as the density threshold and the minimum number of points required to form a cluster.\n\nIn summary, K-Means is suitable for well-defined, spherical clusters with a predetermined number of clusters, while DBSCAN is more flexible, handling clusters of arbitrary shapes and automatically determining the number of clusters based on data density. The choice between them depends on the characteristics of the data and the desired properties of the clusters.",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/clustering/index.html#footnotes",
    "href": "blogs/clustering/index.html#footnotes",
    "title": "Clustering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Cluster_analysis↩︎\nhttps://en.wikipedia.org/wiki/Cluster_analysis↩︎\nhttps://en.wikipedia.org/wiki/K-means_clustering↩︎\nBeer, Anna; Draganov, Andrew; Hohma, Ellen; Jahn, Philipp; Frey, Christian M.M.; Assent, Ira (6 August 2023). “Connecting the Dots -- Density-Connectivity Distance unifies DBSCAN, k-Center and Spectral Clustering”. Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. ACM. pp. 80–92. doi:10.1145/3580305.3599283. ISBN 9798400701030. S2CID 260499476↩︎\nhttps://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html↩︎\nhttps://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html↩︎\nhttps://github.com/SteffiPeTaffy/machineLearningAZ/blob/master/Machine%20Learning%20A-Z%20Template%20Folder/Part%204%20-%20Clustering/Section%2025%20-%20Hierarchical%20Clustering/Mall_Customers.csv↩︎",
    "crumbs": [
      "Blogs",
      "Clustering"
    ]
  },
  {
    "objectID": "blogs/classification/index.html",
    "href": "blogs/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Image source: https://www.freepik.com",
    "crumbs": [
      "Blogs",
      "Classification"
    ]
  },
  {
    "objectID": "blogs/classification/index.html#introduction",
    "href": "blogs/classification/index.html#introduction",
    "title": "Classification",
    "section": "Introduction",
    "text": "Introduction\n\nClassification as Cognitive Process\nClassification, along with counting and sorting, is a fundamental cognitive process that plays a crucial role in human thinking and understanding. It involves categorizing and organizing information into distinct groups or classes based on shared characteristics, properties, or relationships.\nMany children learn to classify objects even before they talk and walk. For example, one of the earliest games our kids played was distinguishing between different categories of colors and shapes.\n\n\nClassification in Machine Learning\nLike the importance of categorization in human thinking, classification is one of the core components of computer science and applications. It providing a systematic way to categorize and make predictions based on input data, thus, it is esstentially a cornerstone of many machine learning algorithms and applications, for example:\n\nAutomated Decision-Making:\nClassification algorithms automate decision-making by learning patterns from labeled training data. Once trained, these algorithms can classify new, unseen data into predefined categories. This is valuable in applications where quick and accurate decisions are required, such as in fraud detection, customer support, and autonomous systems.\nPersonalization and Recommendation Systems:\nClassification algorithms are widely used in recommendation systems to personalize content and make product recommendations. They classify users into groups based on their preferences and behavior, enabling platforms to suggest items that are likely to be of interest.\nNatural Language Processing (NLP):\nIn NLP, classification is used for sentiment analysis, topic categorization, and spam detection. Classification models can analyze and categorize text data, providing valuable insights for applications in customer feedback analysis, content moderation, and more.\nHealthcare and Diagnostics:\nIn healthcare, classification algorithms assist in diagnostics and treatment planning. They can classify medical images, patient records, and genetic data to support medical professionals in making informed decisions.\nFraud Detection and Security:\nClassification algorithms are critical in fraud detection systems. They can classify transactions or user behavior patterns to identify anomalies and potential fraudulent activities in real-world.\n\n\n\nApplications in Pet Disease Prediction\nGiven the fundamental position of classification algorithms in machine learning theory and applications, in this post, we will learn and demonstrate the basic principles and application process of classification algorithms with the prediction of pet diseases.\nThroughout the entire span of human civilization, there is a long history of companionship with pets. There has been a constant quest to comprehend the well-being of our animals. Especially today, with the high development of social and material civilization, there has been a constant quest to comprehend the well-being of our animals. Given that the animals cannot verbally communicate with us and demonstrate their health status, the demand for veterinary medicine has been raised to understand pets’ health conditions based on their behaviors and symptoms.\nFor those who have a dog may notice their dog has some discomfort, which requires a diagnosis from hospitals. Some of the common diseases in dogs are quite severe and need immediate action, so users could enter the symptoms of the dog into our proposed model, and the model will predict what kind of disease the dog has and provide proper recommendations to take further action.\nIn this case, machine learning/deep learning provides a promising approach to making people learn about the health conditions of their pets with plenty of data collected through various sources1 2.",
    "crumbs": [
      "Blogs",
      "Classification"
    ]
  },
  {
    "objectID": "blogs/classification/index.html#methodology",
    "href": "blogs/classification/index.html#methodology",
    "title": "Classification",
    "section": "Methodology",
    "text": "Methodology\n\nSupervised Learning\nBefore we jump into classification, let’s take a look at a learning process in our real life.\nImagine you’re tackling a new mathematical concept. After working on a problem, you might check the solutions to verify your correctness. As you gain confidence in solving a specific problem type, you’ll eventually refrain from consulting the answers and independently tackle new questions that come your way.\nSimilar to human learning process, in Supervised Learning, the model acquires knowledge through examples too. Together with the input variable, we feed the model with the corresponding accurate labels. During the training phase, the model examines the association between our data and the respective labels, enabling it to discover patterns within the dataset.\nClassification is a form of supervised machine learning in which the model endeavors to forecast the accurate label for a provided input dataset. The model undergoes training with the training data in the classification process, followed by an evaluation using test data, before it is applied to make predictions on novel, unseen data. A simple example could be the email classification. By training some sort of algorithms with lots of emails which labelled as spam or ham (no spam), the models can predict whether a incoming email is spam or ham.\n\n\nClassification vs Regression\nThere two main types of supervised learning algorithms, namely classification and regression. Although, they share lots of common characteristics, they are very different from each other.\n\nThe prediction assignment becomes a classification task when the target variable is distinct. An example of this is discerning the inherent sentiment within a given text.\nThe predictive assignment becomes a regression task when the target variable is continuous. For instance, forecasting a person’s salary based on factors such as their educational background, prior work experience, geographical location, etc.\n\n\n\nBinary vs Multiclass classification\nThere are two main types of classifications:\n\nBinary classification involves categorizing data into two distinct classes or categories. An example can be predicting whether a credit card transaction is fraudulent or not, or determining if a patient has a particular medical condition.\nMulticlass classification deals with sorting data into more than two classes or categories. An example can be recognizing handwritten digits (0-9), classifying different species of animals, or identifying the genre of a book.\n\nWhile some algorithms can be adapted for both tasks, certain algorithms are specifically designed for one type of classification. For instance, logistic regression is commonly used for binary classification, while decision trees and neural networks can handle both binary and multiclass scenarios.\nIn this post, we’re going to use the neural networks to conduct a multiclass classification task, specifically, predict whether a pet has one of the given 12 diseases.\n\n\nNeural Network for Multiclass Classification\nWhen apply neural network to solve a problem, design the network architecture and choice the right loss function are the most important task. In this section, we’re going to introduce the loss function but leave the network architecture design for next section.\nFor multiclass classification in a neural network, a common choice for the loss function is the Categorical Cross Entropy Loss, the equation is as follows:\n\n-\\sum_{i=1}^N\\sum_{c=1}^Cw_c\\log\\frac{exp(x_{n,c})}{\\sum_{i=1}^Cexp(x_{n,i})}y_{n,c}\n\nwhere x is the input, y is the target, w is the weight, C is the number of classes, and N denotes the number of training samples in the minibatch.\nThis equation reflects the negative log-likelihood of the correct class in a classification task, penalizing the model more when it is confidently wrong about a particular class. The softmax operation in the denominator ensures that the values in the exponential term form a valid probability distribution over all classes. The logarithm in the loss function penalizes deviations from the true class probabilities.\n\n\nEvaluation Metrics\nFor classification problems, the precision, recall and F_1 are most common used indicators for model performance evaluation.\nAccording to wikipedia, the definition of these metrics as following3 4:\nPrecision is the fraction of relevant instances among the retrieved instances. Written as a formula:\n\n{\\displaystyle {\\text{Precision}}={\\frac {\\text{Relevant retrieved instances}}{\\text{All retrieved instances}}}}\n\nRecall is the fraction of relevant instances that were retrieved. Written as a formula:\n\n{\\displaystyle {\\text{Recall}}={\\frac {\\text{Relevant retrieved instances}}{\\text{All relevant instances}}}}\n\nF_1 score is the harmonic mean of the precision and recall. Written as a formula:\n\n{\\displaystyle F_{1}={\\frac {2}{\\mathrm {recall} ^{-1}+\\mathrm {precision} ^{-1}}}=2{\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}={\\frac {2\\mathrm {tp} }{2\\mathrm {tp} +\\mathrm {fp} +\\mathrm {fn} }}}\n\nwhere tp is the true positives, fp is the false positive, and fn is the false negative.",
    "crumbs": [
      "Blogs",
      "Classification"
    ]
  },
  {
    "objectID": "blogs/classification/index.html#datasets",
    "href": "blogs/classification/index.html#datasets",
    "title": "Classification",
    "section": "Datasets",
    "text": "Datasets\n\nDescription\nIn this post, we will use a dog disease dataset5 containing 24000 entities with 17 symptoms and 12 diseases to conduct experiments. The dataset is in csv format, the symptoms and diseases are presented in text format, thus, we need to further convert the textual information into numerical representations. However, due to the length of this post, we’re not able to cover the feature engineering processing here, instead, we will focus on the data analysis and modeling process, which would be introduced in the next section.\n\n\nData Analysis\nWe first load all the python libraries we might use in the analysis:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nNow, load the prepossessed (convert the textual data into numerical representation) csv file into pandas DataFrame:\n\ndf = pd.read_csv('data/pre_processed.csv')\n\nLet’s take a look at the first 5 rows:\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nFever\nNasal Discharge\nLoss of appetite\nWeight Loss\nLameness\nBreathing Difficulty\nSwollen Lymph nodes\nLethargy\nDepression\n...\nHepatitis\nTetanus\nChronic kidney Disease\nDiabetes\nGastrointestinal Disease\nAllergies\nGingitivis\nCancers\nSkin Rashes\nDisease\n\n\n\n\n0\n0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nTick fever\n\n\n1\n1\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nTick fever\n\n\n2\n2\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nTick fever\n\n\n3\n3\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nTick fever\n\n\n4\n4\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nTick fever\n\n\n\n\n5 rows × 100 columns\n\n\n\n\nAs we can see in the table above, there are 98 features, and all the features are categorical which represents whether the dog has a certain symptom. The column Disease is the type of disease we want to predict.\n\nDescriptive Statistics\nWe now take a look at the descriptive statistics of the dataset:\n\ndf.describe()\n\n\n\n\n\n\n\n\nUnnamed: 0\nFever\nNasal Discharge\nLoss of appetite\nWeight Loss\nLameness\nBreathing Difficulty\nSwollen Lymph nodes\nLethargy\nDepression\n...\nParvovirus\nHepatitis\nTetanus\nChronic kidney Disease\nDiabetes\nGastrointestinal Disease\nAllergies\nGingitivis\nCancers\nSkin Rashes\n\n\n\n\ncount\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n...\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n23999.000000\n\n\nmean\n11999.000000\n0.104129\n0.106171\n0.282178\n0.142214\n0.072378\n0.146839\n0.035918\n0.286887\n0.138631\n...\n0.083337\n0.083295\n0.083337\n0.083337\n0.083337\n0.083337\n0.083337\n0.083337\n0.083337\n0.083337\n\n\nstd\n6928.058891\n0.305435\n0.308063\n0.450069\n0.349277\n0.259118\n0.353953\n0.186090\n0.452318\n0.345568\n...\n0.276396\n0.276334\n0.276396\n0.276396\n0.276396\n0.276396\n0.276396\n0.276396\n0.276396\n0.276396\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n5999.500000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n11999.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n17998.500000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n23998.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 99 columns\n\n\n\nFrom the table above, we can infer that the features are very sparse with most of them are zeros, this requires the model have the ability to effectively extract information from high sparse data.\n\n\nVisualization\nWe wanted to examine the dependent variable first, this could be done by count how many records for each disease. We plot the histogram as follows:\n\nsns.histplot(df['Disease'])\nplt.xticks(rotation=75)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the figure above, we can see that there are 12 diseases and there are equally distributed.\nIt would be interesting to show the distribution of the symptoms, we thus plots a word cloud of the symptom names as following:\n\ncount_series = df.drop(columns=[\"Unnamed: 0\", \"Disease\"]).sum(axis = 0)\ncount_df = count_series.to_frame().reset_index().rename(columns = {\"index\":\"symptom\", 0:\"frequency\"})\nfreq_list = {}\nfor index in range(len(count_df)):\n    freq_list[count_df['symptom'].iloc[index]] = count_df['frequency'].iloc[index]\n\n\nfrom wordcloud import WordCloud\nwc = WordCloud(width=3840, height=2160).generate_from_frequencies(freq_list)\nplt.figure(figsize=(32, 10))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nData Preprocessing\nWe then, further convert the pandas dataframe to the format fits the model training requirements. It mainly includes: separate the features and labels into different numpy array; split the data into train, validation and test set.\n\nX = df.drop(['Unnamed: 0', 'Disease','Tick fever','Distemper', 'Parvovirus',\n               'Hepatitis', 'Tetanus', 'Chronic kidney Disease', 'Diabetes',\n               'Gastrointestinal Disease', 'Allergies', 'Gingitivis', 'Cancers',\n               'Skin Rashes'], axis=1)\ny = df[['Tick fever', 'Distemper', 'Parvovirus',\n          'Hepatitis', 'Tetanus', 'Chronic kidney Disease', 'Diabetes',\n          'Gastrointestinal Disease', 'Allergies', 'Gingitivis', 'Cancers',\n          'Skin Rashes']]\n\n\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=66)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=66)\n\n\n# Show how many data in each set\nprint(\"Length of train set:\", len(X_train))\nprint(\"Length of validation set:\", len(X_val))\nprint(\"Length of test set:\", len(X_test))\n\nLength of train set: 14399\nLength of validation set: 4800\nLength of test set: 4800",
    "crumbs": [
      "Blogs",
      "Classification"
    ]
  },
  {
    "objectID": "blogs/classification/index.html#experiments",
    "href": "blogs/classification/index.html#experiments",
    "title": "Classification",
    "section": "Experiments",
    "text": "Experiments\nWe will use the pytorch framework to build and train a neural network, thus, we need to use the pytorch ecosystem to format the dataset, build the network architecture, load and feed dataset model, etc. We will introduce all these steps in this section.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n\nNetwork Implementation\nWrap the dataset into pytorch Dataset format:\n\nclass DogSymptomsDataset(Dataset):\n  def __init__(self, X, Y):\n    self.X = torch.tensor(X.values, dtype=torch.float32)\n    self.Y = torch.tensor(Y.values, dtype=torch.float32)\n\n  def __len__(self):\n    return len(self.X)\n\n  def __getitem__(self, index):\n    return (self.X[index], self.Y[index])\n\nInitialize the DogSymptomsDataset and DataLoader for the train, validation and test data.\n\ntrain_ds = DogSymptomsDataset(X_train, y_train)\nval_ds = DogSymptomsDataset(X_val, y_val)\ntest_ds = DogSymptomsDataset(X_test, y_test)\n\ntrainloader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\nvalloader = DataLoader(val_ds, batch_size=32, shuffle=True, num_workers=2)\ntestloader = DataLoader(test_ds, batch_size=32, shuffle=True, num_workers=2)\n\nThe network structure mainly consist of several feedforward and dropoutas layer, we use ReLU as the activation function and implement the forward function for passing the data through network. The pytorch framework will take care of error back-propagation.\n\nclass MLP(nn.Module):\n  def __init__(self):\n    super(MLP, self).__init__()\n    self.layers = nn.Sequential(\n        nn.Linear(86, 256),\n        nn.Dropout(0.2),\n        nn.ReLU(),\n        nn.Linear(256, 128),\n        nn.Dropout(0.2),\n        nn.ReLU(),\n        nn.Linear(128, 64),\n        nn.Dropout(0.2),\n        nn.ReLU(),\n        nn.Linear(64, 12),\n        # nn.Softmax()\n\n    )\n  def forward(self, x):\n    return self.layers(x)\n\n\n\nTraining\nThe last missing puzzle now is implement the train and validation function which manages the whole training and validation process.\n\ndef f1(pred_list, label_list):\n  pred_all = torch.cat(pred_list)\n  label_all = torch.cat(label_list)\n  y_pred_classes = np.argmax(pred_all.detach().numpy(), axis=1)\n  y_true_classes = np.argmax(label_all.detach().numpy(), axis=1)\n  _f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')*100\n  return _f1\n\ndef validation(model):\n  pred_list = []\n  label_list = []\n  loss_list = []\n  loss_function = nn.CrossEntropyLoss()\n  for i, data in enumerate(valloader):\n      # Get inputs\n      inputs, targets = data\n      # Perform forward pass\n      outputs = model(inputs)\n      loss = loss_function(outputs, targets)\n      loss_list.append(loss.item())\n      pred_list.append(outputs)\n      label_list.append(targets)\n\n  avg_loss = sum(loss_list)/len(valloader)\n  avg_f1 = f1(pred_list, label_list)\n  return (avg_loss, avg_f1)\n\n\ndef train():\n  mlp = MLP()\n  torch.manual_seed(666)\n  trainloader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n  loss_function = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(mlp.parameters(), lr=2e-4)\n  train_loss_list = []\n  val_loss_list = []\n  train_f1_list=[]\n  val_f1_list=[]\n  for epoch in range(10):\n    # Print epoch\n    print(f'Starting epoch {epoch+1}')\n    loss_epoch_list = []\n    pred_list = []\n    label_list = []\n    current_loss = 0.0\n    # Iterate over the DataLoader for training data\n    for i, data in enumerate(trainloader):\n      # Get inputs\n      inputs, targets = data\n      # Zero the gradients\n      optimizer.zero_grad()\n      # Perform forward pass\n      outputs = mlp(inputs)\n      pred_list.append(outputs)\n      label_list.append(targets)\n      # Compute loss\n      loss = loss_function(outputs, targets)\n      # Perform backward pass\n      loss.backward()\n      # Perform optimizationd\n      optimizer.step()\n\n      # Print statistics\n      loss_item = loss.item()\n      loss_epoch_list.append(loss_item)\n      current_loss += loss_item\n      if (i !=0) and (i % 50 == 0):\n          train_loss_list.append(current_loss/50)\n          train_f1 = f1([outputs], [targets])\n          train_f1_list.append(train_f1)\n\n          val_loss, val_f1 = validation(mlp)\n          val_loss_list.append(val_loss)\n          val_f1_list.append(val_f1)\n\n          print('Loss after mini-batch %5d: %.3f' %\n                (i, current_loss/50))\n          current_loss = 0.0\n  print('Training process has finished.')\n  return (mlp, train_loss_list,val_loss_list,train_f1_list,val_f1_list)\n\nWe now start the training process:\n\nmodel, train_loss_list,val_loss_list,train_f1_list,val_f1_list = train()\n\n\n\nPerformance Evaluation\nWe are now have already completed the model training, it is time to use the model to make predict on our test set, then, calculate the precision, recall and F_1 score base on the predicted disease and the ground truth label.\n\ndef test(model):\n  pred_list = []\n  label_list = []\n  for i, data in enumerate(testloader):\n      # Get inputs\n      inputs, targets = data\n      # Perform forward pass\n      outputs = model(inputs)\n      pred_list.append(outputs)\n      label_list.append(targets)\n  pred_all = torch.cat(pred_list)\n  label_all = torch.cat(label_list)\n  y_pred_classes = np.argmax(pred_all.detach().numpy(), axis=1)\n  y_true_classes = np.argmax(label_all.detach().numpy(), axis=1)\n  avg_f1 = f1(pred_list, label_list)\n  return (avg_f1, y_pred_classes, y_true_classes)\n\n\navg_f1, y_pred_classes, y_true_classes = test(model)\nreport = classification_report(y_true_classes, y_pred_classes,output_dict=True)\nreport_labeled={}\ndnames = df['Disease'].unique().tolist()\nfor i in range(len(dnames)):\n  report_labeled[dnames[i]] = report[str(i)]\nreport_labeled['macro avg'] = report['macro avg']\nreport_labeled['weighted avg'] = report['weighted avg']\npd.DataFrame(report_labeled).T\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\nTick fever\n0.995074\n0.990196\n0.992629\n408.0\n\n\nDistemper\n0.992500\n1.000000\n0.996236\n397.0\n\n\nParvovirus\n1.000000\n1.000000\n1.000000\n411.0\n\n\nHepatitis\n1.000000\n0.989822\n0.994885\n393.0\n\n\nTetanus\n0.997494\n1.000000\n0.998745\n398.0\n\n\nChronic kidney Disease\n0.997468\n0.997468\n0.997468\n395.0\n\n\nDiabetes\n0.994898\n1.000000\n0.997442\n390.0\n\n\nGastrointestinal Disease\n1.000000\n0.997519\n0.998758\n403.0\n\n\nAllergies\n1.000000\n1.000000\n1.000000\n406.0\n\n\nGingitivis\n1.000000\n1.000000\n1.000000\n385.0\n\n\nCancers\n0.995134\n0.997561\n0.996346\n410.0\n\n\nSkin Rashes\n1.000000\n1.000000\n1.000000\n404.0\n\n\nmacro avg\n0.997714\n0.997714\n0.997709\n4800.0\n\n\nweighted avg\n0.997715\n0.997708\n0.997707\n4800.0\n\n\n\n\n\n\n\nAs we can see from the table above, our neural network predicts the 12 disease at very high precision, recall and F_1 scores.\nWe can also plot the confusion matrix to have better understanding of the prediction performance:\n\nconf_matrix = metrics.confusion_matrix(y_true_classes, y_pred_classes)\ndf_cm = pd.DataFrame(conf_matrix, index=df['Disease'].unique(), columns=df['Disease'].unique())\nprint('F1-score% =', f1_score(y_true_classes, y_pred_classes, average='weighted')*100, '|',\n      'Accuracy% =', accuracy_score(y_true_classes, y_pred_classes)*100, '|',\n      'Recall% =', recall_score(y_true_classes, y_pred_classes, average='weighted')*100, '|',\n      'Precision% =', precision_score(y_true_classes, y_pred_classes, average='weighted')*100\n      )\nplt.figure(figsize = (8,6))\nsns.heatmap(df_cm)\nplt.show()\n\nF1-score% = 99.77067574596379 | Accuracy% = 99.77083333333333 | Recall% = 99.77083333333333 | Precision% = 99.77146300647077\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(conf_matrix.shape[0]):\n    for j in range(conf_matrix.shape[1]):\n        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n\nplt.xlabel('Predictions', fontsize=18)\nplt.ylabel('Actuals', fontsize=18)\nplt.title('Confusion Matrix of ANN Model', fontsize=18)\nplt.show()\n\n\n\n\n\n\n\n\nFurther, we can visualize the the loss of the train and validation split during the whole training process, which is shown as follows:\n\ntrain_loss = train_loss_list\nval_loss = val_loss_list\ntrain_acc = train_f1_list\nval_acc = val_f1_list\n\n# Draw loss plot\nplt.figure(figsize = (8,8))\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.xlabel('Batch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss')\nplt.show()\n\n\n\n\n\n\n\n\nThe F_1 score of the train and test split during the whole training process is shown as follows:\n\n# Draw accuracy plot\nplt.figure(figsize = (8,8))\nplt.plot(train_acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.xlabel('Batch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy')\nplt.show()",
    "crumbs": [
      "Blogs",
      "Classification"
    ]
  },
  {
    "objectID": "blogs/classification/index.html#discussion-and-conclusions",
    "href": "blogs/classification/index.html#discussion-and-conclusions",
    "title": "Classification",
    "section": "Discussion and Conclusions",
    "text": "Discussion and Conclusions\nIn this blog, we begin by introducing the basic concepts of classification and its importance in computer science and applications. Secondly, we introduce the background knowledge about classification such as supervised learning, the connection and difference between classification and regression, and the difference between binary and multiclass classification. After introducing the pet disease prediction use case, we provide a detailed description of the use of neural networks in multiclass classification, as well as the related evaluation indexes.\nAfter a detailed analysis of the data, we modeled the data using neural networks, and carried out an evaluation of the trained model. The experiment result shows our model achieved F_1 score of over 99% in all 12 pet disease predictions.\nWhile neural networks are powerful tools for many classification tasks, they are not without their shortcomings. Here are some common limitations and challenges associated with using neural networks for classification:\n\nData Requirements:\nNeural networks often require large amounts of labeled data for training to generalize well. In situations where labeled data is scarce, the model’s performance may suffer.\nComputational Complexity:\nTraining deep neural networks can be computationally intensive, especially with large datasets and complex architectures. This can be a limitation for applications with resource constraints.\nInterpretability: Neural networks, particularly deep architectures, are often considered as “black-box” models, making it challenging to interpret and understand the decision-making process. Lack of interpretability can be a concern in fields where transparency and explainability are crucial.\n\nDespite these challenges, ongoing research and advancements in deep learning aim to address these limitations and enhance the performance, robustness, and interpretability of neural networks. Let’s keep learning!",
    "crumbs": [
      "Blogs",
      "Classification"
    ]
  },
  {
    "objectID": "blogs/classification/index.html#footnotes",
    "href": "blogs/classification/index.html#footnotes",
    "title": "Classification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHwang, S., Shin, H. K., Park, J. M., Kwon, B., & Kang, M. G. (2022). Classification of dog skin diseases using deep learning with images captured from multispectral imaging devices. Molecular & Cellular Toxicology, 18(3), 299-309.↩︎\nDebes, C., Wowra, J., Manzoor, S. et al. Predicting health outcomes in dogs using insurance claims data. Sci Rep 13, 9122 (2023). https://doi.org/10.1038/s41598-023-36023-5↩︎\nhttps://en.wikipedia.org/wiki/Precision_and_recall↩︎\nhttps://en.wikipedia.org/wiki/F-score↩︎\nhttps://github.com/1zuu/Doggy-Disease-Detection/↩︎",
    "crumbs": [
      "Blogs",
      "Classification"
    ]
  },
  {
    "objectID": "blogs/welcome/index.html",
    "href": "blogs/welcome/index.html",
    "title": "Welcome to Machine Learning Blog",
    "section": "",
    "text": "Image source: https://www.freepik.com",
    "crumbs": [
      "Blogs",
      "Welcome to Machine Learning Blog"
    ]
  },
  {
    "objectID": "blogs/welcome/index.html#welcome",
    "href": "blogs/welcome/index.html#welcome",
    "title": "Welcome to Machine Learning Blog",
    "section": "Welcome",
    "text": "Welcome\nThis is the first post in my Machine Learning blog series. Welcome!",
    "crumbs": [
      "Blogs",
      "Welcome to Machine Learning Blog"
    ]
  },
  {
    "objectID": "blogs/welcome/index.html#powered-by-quarto",
    "href": "blogs/welcome/index.html#powered-by-quarto",
    "title": "Welcome to Machine Learning Blog",
    "section": "Powered by Quarto",
    "text": "Powered by Quarto\nThanks to Dr. Martin Laptev’s recommendation, I was introduced to Quarto. Quarto is sooooo… cool, can’t imagine I configured the environment and created my first tech blog built from scratch in minutes, AMAZING!",
    "crumbs": [
      "Blogs",
      "Welcome to Machine Learning Blog"
    ]
  },
  {
    "objectID": "blogs/welcome/index.html#topics",
    "href": "blogs/welcome/index.html#topics",
    "title": "Welcome to Machine Learning Blog",
    "section": "Topics",
    "text": "Topics\nI will share my opinions, experiences and knowledge here from time to time.\nI am very excited and look forward to the blogging journey ahead.",
    "crumbs": [
      "Blogs",
      "Welcome to Machine Learning Blog"
    ]
  },
  {
    "objectID": "news/second-news/index.html",
    "href": "news/second-news/index.html",
    "title": "Tong Zeng",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.\n\n\n\n\n\n\n\n\nAdd a README.md file to present the steps how to restore the project enviroment after operation system reinstall (90c35ef86)\n\n\n\n\n\nTextual content in the index.qmd and about.qmd\n\n\n\n\n\n\n\n\nAdd project render target config to exclude CHANGELOG.md from rendering (735b8a60)\nAdd giscus to the blog posts (789dfc13)\n\n\n\n\n\n\n\n\nAdd Google Analytics through Google Tag Manager (92fa14b0)\nAdd responsive layout for featured publications section (e241aff7)\nAdd responsive layout for intro section and pub list section (9f8f1a12)\ntext-decoration-thickness set to 0.2rem (ac1f833b)\nAdd robots.txt and sitemap.xml (ebe70678)\n\n\n\n\n\n\n\n\nFirst version of the website have the custom listing, publications posts, about page ready. (bf30121e)"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "Changelog",
    "section": "",
    "text": "Add a README.md file to present the steps how to restore the project enviroment after operation system reinstall (90c35ef86)\n\n\n\n\n\nTextual content in the index.qmd and about.qmd"
  },
  {
    "objectID": "CHANGELOG.html#section-1",
    "href": "CHANGELOG.html#section-1",
    "title": "Changelog",
    "section": "",
    "text": "Add project render target config to exclude CHANGELOG.md from rendering (735b8a60)\nAdd giscus to the blog posts (789dfc13)"
  },
  {
    "objectID": "CHANGELOG.html#section-2",
    "href": "CHANGELOG.html#section-2",
    "title": "Changelog",
    "section": "",
    "text": "Add Google Analytics through Google Tag Manager (92fa14b0)\nAdd responsive layout for featured publications section (e241aff7)\nAdd responsive layout for intro section and pub list section (9f8f1a12)\ntext-decoration-thickness set to 0.2rem (ac1f833b)\nAdd robots.txt and sitemap.xml (ebe70678)"
  },
  {
    "objectID": "CHANGELOG.html#section-3",
    "href": "CHANGELOG.html#section-3",
    "title": "Changelog",
    "section": "",
    "text": "First version of the website have the custom listing, publications posts, about page ready. (bf30121e)"
  }
]