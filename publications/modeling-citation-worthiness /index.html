<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.537">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2020-03-28">
<meta name="keywords" content="Citation worthiness, Citation context, Deep learning">
<meta name="description" content="Predict whether a sentence needs citations using Att-BiLSTM Networks, Logistic Regression and Random Forest.">

<title>Tong Zeng - Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html" rel="prev">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta name="citation_title" content="Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models">
<meta name="citation_abstract" content="Scientist learn early on how to cite scientific sources to support their claims. Sometimes, however, scientists have challenges determining where a citation should be situated—or, even worse, fail to cite a source altogether. Automatically detecting sentences that need a citation (i.e., citation worthiness) could solve both of these issues, leading to more robust and well-constructed scientific arguments. Previous researchers have applied machine learning to this task but have used small datasets and models that do not take advantage of recent algorithmic developments such as attention mechanisms in deep learning. We hypothesize that we can develop significantly accurate deep learning architectures that learn from large supervised datasets constructed from open access publications. In this work, we propose a bidirectional long short-term memory network with attention mechanism and contextual information to detect sentences that need citations. We also produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset, which is orders of magnitude larger than previous datasets. Our experiments show that our architecture achieves state of the art performance on the standard ACL-ARC dataset ( F1 = 0.507) and exhibits high performance ( F1 = 0.856) on the new PMOA-CITE. Moreover, we show that it can transfer learning across these datasets. We further use interpretable models to illuminate how specific language is used to promote and inhibit citations. We discover that sections and surrounding sentences are crucial for our improved predictions. We further examined purported mispredictions of the model, and uncovered systematic human mistakes in citation behavior and source data. This opens the door for our model to check documents during pre-submission and pre-archival procedures. We discuss limitations of our work and make this new dataset, the code, and a web-based tool available to the community.
">
<meta name="citation_keywords" content="Citation worthiness,Citation context,Deep learning">
<meta name="citation_author" content="Tong Zeng">
<meta name="citation_author" content="Daniel E Acuna">
<meta name="citation_publication_date" content="2020-07-01">
<meta name="citation_cover_date" content="2020-07-01">
<meta name="citation_year" content="2020">
<meta name="citation_online_date" content="2020-03-28">
<meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s11192-020-03421-9">
<meta name="citation_pdf_url" content="./pdf/citation-worthiness-att-bilstm.pdf">
<meta name="citation_issue" content="1">
<meta name="citation_doi" content="10.1007/s11192-020-03421-9">
<meta name="citation_issn" content="0138-9130">
<meta name="citation_eissn" content="1588-2861">
<meta name="citation_volume" content="124">
<meta name="citation_language" content="en">
<meta name="citation_firstpage" content="399">
<meta name="citation_lastpage" content="428">
<meta name="citation_journal_title" content="Scientometrics">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Tong Zeng</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../#news"> 
<span class="menu-text">News</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../#publications"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blogs.html"> 
<span class="menu-text">Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tong-zeng/tong-zeng.github.io"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/zeng_tong"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:tongzeng@vt.edu"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <a class="flex-grow-1 no-decor" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
          <h1 class="quarto-secondary-nav-title">Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models</h1>
        </a>     
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block">Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models</h1>
                  <div>
        <div class="description">
          Predict whether a sentence needs citations using Att-BiLSTM Networks, Logistic Regression and Random Forest.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Science of Science</div>
                <div class="quarto-category">Deep learning</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-heading">Affiliations</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Tong Zeng <a href="mailto:tongzeng@vt.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              School of Information Management, Nanjing University, Nanjing 210023, China
            </p>
          <p class="affiliation">
              School of Information Studies, Syracuse University, Syracuse, NY 13244, USA
            </p>
        </div>
      <div class="quarto-title-meta-contents">
      <p class="author">Daniel E Acuna <a href="mailto:daniel.acuna@colorado.edu" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-7765-1595" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              School of Information Studies, Syracuse University, Syracuse, NY 13244, USA
            </p>
        </div>
    </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 28, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">Abstract</div>
      <p>Scientist learn early on how to cite scientific sources to support their claims. Sometimes, however, scientists have challenges determining where a citation should be situated—or, even worse, fail to cite a source altogether. Automatically detecting sentences that need a citation (i.e., citation worthiness) could solve both of these issues, leading to more robust and well-constructed scientific arguments. Previous researchers have applied machine learning to this task but have used small datasets and models that do not take advantage of recent algorithmic developments such as attention mechanisms in deep learning. We hypothesize that we can develop significantly accurate deep learning architectures that learn from large supervised datasets constructed from open access publications. In this work, we propose a bidirectional long short-term memory network with attention mechanism and contextual information to detect sentences that need citations. We also produce a new, large dataset (PMOA-CITE) based on PubMed Open Access Subset, which is orders of magnitude larger than previous datasets. Our experiments show that our architecture achieves state of the art performance on the standard ACL-ARC dataset ( F1 = 0.507) and exhibits high performance ( F1 = 0.856) on the new PMOA-CITE. Moreover, we show that it can transfer learning across these datasets. We further use interpretable models to illuminate how specific language is used to promote and inhibit citations. We discover that sections and surrounding sentences are crucial for our improved predictions. We further examined purported mispredictions of the model, and uncovered systematic human mistakes in citation behavior and source data. This opens the door for our model to check documents during pre-submission and pre-archival procedures. We discuss limitations of our work and make this new dataset, the code, and a web-based tool available to the community.</p>
    </div>
  </div>

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Citation worthiness, Citation context, Deep learning</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Publications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../publications/assigning-credit-sci-dataset/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assigning credit to scientific datasets using article citation networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../publications/large-scale-author-name-disambiguation/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Large-scale author name disambiguation using approximate network structures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Dead Science: Most Resources Linked in Biomedical Articles Disappear in Eight Years</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../publications/modeling-citation-worthiness /index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Modeling citation worthiness by using attention‑based bidirectional long short‑term memory networks and interpretable models</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#data-sources-and-data-preprocessing" id="toc-data-sources-and-data-preprocessing" class="nav-link" data-scroll-target="#data-sources-and-data-preprocessing"><span class="header-section-number">2</span> Data sources and data pre‑processing</a>
  <ul>
  <li><a href="#pmoa-cite" id="toc-pmoa-cite" class="nav-link" data-scroll-target="#pmoa-cite"><span class="header-section-number">2.1</span> PMOA-CITE</a></li>
  </ul></li>
  <li><a href="#text-representation" id="toc-text-representation" class="nav-link" data-scroll-target="#text-representation"><span class="header-section-number">3</span> Text Representation</a>
  <ul>
  <li><a href="#bag-of-words-bow-representation" id="toc-bag-of-words-bow-representation" class="nav-link" data-scroll-target="#bag-of-words-bow-representation"><span class="header-section-number">3.1</span> Bag of words (BoW) representation</a></li>
  <li><a href="#topic-modeling-based-tm-representation" id="toc-topic-modeling-based-tm-representation" class="nav-link" data-scroll-target="#topic-modeling-based-tm-representation"><span class="header-section-number">3.2</span> Topic modeling based (TM) representation</a></li>
  <li><a href="#distributed-word-representation" id="toc-distributed-word-representation" class="nav-link" data-scroll-target="#distributed-word-representation"><span class="header-section-number">3.3</span> Distributed word representation</a></li>
  </ul></li>
  <li><a href="#an-attentionbased-bilstm-architecture-for-citation-worthiness" id="toc-an-attentionbased-bilstm-architecture-for-citation-worthiness" class="nav-link" data-scroll-target="#an-attentionbased-bilstm-architecture-for-citation-worthiness"><span class="header-section-number">4</span> An attention‑based BiLSTM architecture for citation worthiness</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">5</span> Results</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">6</span> Discussion</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/tong-zeng/tong-zeng.github.io/blob/main/publications/modeling-citation-worthiness /index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div class="paper-access-btns">
    <a href="./pdf/citation-worthiness-att-bilstm.pdf" target="_blank" class="btn rounded-0 btn-primary btn-lg page-scroll">Download Preprint PDF</a>
    <a href="https://doi.org/10.1016/j.joi.2020.101013" target="_blank" class="btn rounded-0 btn-lg btn-outline-primary page-scroll">Visit Published DOI</a>
    <a href="https://doi.org/10.6084/m9.figshare.12547574" target="_blank" class="btn rounded-0 btn-outline-primary btn-lg page-scroll">PMOA-CITE dataset</a>
    <a href="https://github.com/sciosci/cite-worthiness" target="_blank" class="btn rounded-0 btn-outline-primary btn-lg page-scroll">Code</a>
    <a href="http://cite-worthiness.scienceofscience.org" target="_blank" class="btn rounded-0 btn-outline-primary btn-lg page-scroll">Online Demo</a>
</div>
<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Scientists and journalists have challenges determining proper citations in the ever increasing sea of information. More fundamentally, when and where a citation is needed—sometimes called citation worthiness—is a crucial first step to solve this challenge. In the general media, some problematic stories have shown that claims need citations to make them verifiable—e.g., <em>the debunked A Rape on Campus</em> article in the Rolling Stone magazine (<code>Wikipedia contributors 2018</code>). Analyses of Wikipedia have revealed that lack of citations correlates with an article’s immaturity (<code>Jack et al. 2014</code>; <code>Chen and Roth 2012</code>). In science, the lack of citations leaves readers wondering how results were built upon previous work (<code>Aksnes and Rip 2009</code>). Also, it precludes researchers from getting appropriate credit, important during hiring and promotion (<code>Gazni and Ghaseminik 2016</code>). The sentences surrounding a citation provide rich information for common semantic analyses, such as information retrieval (<code>Nakov et al. 2004</code>). There should be methods and tools to help scientists cite; in this work, we want to understand where citations should be situated in a paper with the goal of automatically suggesting them.</p>
<p>Relatively much less work has been done on detecting where a citation should be. He et al.&nbsp;(<code>2011</code>) were the first to introduce the task of identifying candidate location where citations are needed in the context of scientific articles. Jack et al.&nbsp;(<code>2014</code>) studied how to detect citation needs in Wikipedia. Peng et al.&nbsp;(<code>2016</code>) used the learning-to-rank framework to solve citation recommendation in news articles. These are very diverse domains, and therefore it is difficult to generalize results. We contend that a large standard dataset of citation location with open code and services would significantly improve the systematic study of the problem. Thus, the task of citation worthiness detection is relatively new and needs further exploration.</p>
<p>The attention mechanism is a relatively recent development in neural networks motivated by human visual attention. Humans get more information from the region they pay attention to, and perceive less from other regions. An attention mechanism in neural networks was first introduced in computer vision (<code>Sun and Fisher 2003</code>), and later applied to NLP for machine translation (<code>Bahdanau et al. 2014</code>). Attention has quickly become adopted in other sub-domains. Luong et al.&nbsp;(<code>2015</code>) examined several attention scoring functions for machine translation. Li et al.&nbsp;(<code>2016</code>) used attention mechanisms to improve results in a question-answering task. Zhou et al.&nbsp;(<code>2016</code>) made use of an attention-based LSTM network to do relational classification. Lin et al.&nbsp;(<code>2017</code>) used attention to improve sentence embedding. Recently, Vaswani et al.&nbsp;(<code>2017</code>) built an architecture called transformer that promises to replace recurrent neural networks (RNNs) altogether by only using attention mechanisms. These results show the advantage of attention for NLP tasks and thus its potential benefit for citation worthiness.</p>
<p>In this study, we formulate the detection of sentences that need citations as a classification task that can be effectively solved with a deep learning architecture that relies on an attention mechanism. Our contributions are the following:</p>
<ol type="1">
<li>A deep learning architecture based on bidirectional LSTM with attention and contextual information for citation worthiness</li>
<li>A new large scale dataset for the citation worthiness task that is 300 times bigger that the next current alternative</li>
<li>A set of classic interpretable models that provide insights into the language used for making citations</li>
<li>An examination of common citation mistakes—from unintentional omissions to poten- tially problematic mis-citations</li>
<li>An evaluation of transfer learning between our proposed dataset and the ACL-ARC dataset</li>
<li>The code to produce the dataset and results, a web-based tool for the community to evaluate our predictions, and the pre-processed dataset.</li>
</ol>
</section>
<section id="data-sources-and-data-preprocessing" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="data-sources-and-data-preprocessing"><span class="header-section-number">2</span> Data sources and data pre‑processing</h2>
<section id="pmoa-cite" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="pmoa-cite"><span class="header-section-number">2.1</span> PMOA-CITE</h3>
<p>In this paper, We constructed a new dataset called <code>PMOA-CITE</code> based on PubMed central open access subset.</p>
<p>PubMed Central Open Access Subset (PMOAS) is a full-text collection of scientific literature in bio-medical and life sciences. PMOAS is created by the US’s National Institutes of Health. We obtain a snapshot of PMOAS on August, 2019. The dataset consists of more than 2 million full-text journal articles organized in well-structured XML files by the National Information Standards Organization (<code>ANSI/NISO 2013</code>).</p>
<p>We prepare the <code>PMOA-CITE</code> in the following steps:</p>
<ol type="1">
<li><p>Sentence segmentation and outlier removal. Text in a PMOAS XML file is marked by a paragraph tag, but there might be other XML tags inside paragraph tags. Therefore, we needed to get all the text of a paragraph from XML tags recursively and break paragraphs into sentences. We used spaCy Python package to do the sentence splitting (<code>Honnibal and Montani 2017</code>). However, there are some outliers in the sentences (e.g., long gene sequences with more then 10 thousand characters that are treated as one sentence). Base on the distribution of sentence length (see Fig. 3), we remove the sentences that are outliers either in character or word length. We winsorize 5% and 95% quantiles. For character-wise length, this amounts to 19 characters for 5% quantile and 275 characters for 95% quantile. For word-wise length, it is 3 words and 42 words, respectively.</p></li>
<li><p>Hierarchical tree-like structure. By using section and paragraph tagging information in the XML file and the sentences we extracted in previous step, we construct a hierarchical tree-like structure of the articles. In this structure, sentences are contained within paragraphs, which in turn are contained within sections. For each section, we extract the section-type attribute from the XML file which indicates which kind of section is (from a pre-defined set). For those sections without a section-type, we use the section title instead.</p></li>
<li><p>Citation hints removal. The citing sentence usually has some explicit hints which discloses a citation. This provides too much information for the model training and it does not faithfully represents a real-world application scenario. Thus, we removed all the citation hints by regular expression (see <a href="#tbl-expression-to-remove-hints" class="quarto-xref">Table&nbsp;1</a>).</p></li>
</ol>
<div id="tbl-expression-to-remove-hints" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-expression-to-remove-hints-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Regular expression to remove the citation hints
</figcaption>
<div aria-describedby="tbl-expression-to-remove-hints-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 40%">
<col style="width: 27%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>Regular expression</th>
<th style="text-align: left;">Description</th>
<th style="text-align: right;">Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>(?&lt;!^)([\[\(])[\s]* ([\d][\s\,\-\–\;\-]*)* [\d][\s]*[\]\)]</code></td>
<td style="text-align: left;">numbers contained in parentheses and square rackets</td>
<td style="text-align: right;">“[1, 2]”, “[ 1- 2]”, “(1-3)”, “(1,2,3)”, “[1-3, 5]”, “[8],[9],[12]”, “( 1-2; 4-6; 8 )”</td>
</tr>
<tr class="even">
<td><code>[\(\[]\s*([^\(\)\[\]]* (((16|17|18|19|20) \d{2}(?!\d))| (et[\.&nbsp;\s\\xa0]*al\.)) [^\(\)]*)?[\)\]]</code></td>
<td style="text-align: left;">text within parentheses</td>
<td style="text-align: right;">“(Kim and li, 2008)”, “(Heijman , 2013b)”, “(Tárraga , 2006; Capella-Gutiérrez , 2009)”, “(Kobayashi et al., 2005)”, “(Richart and Barron, 1969; Campion et al, 1986)”, “(Nasiell et al, 1983, 1986)”</td>
</tr>
<tr class="odd">
<td><code>et[\.&nbsp;\s\\xa0]+al[\.\s\(\[]* ((16|17|18|19|20)\d {2})*[)\] \s]*(?=\D)</code></td>
<td style="text-align: left;">remove et al.&nbsp;and the following years</td>
<td style="text-align: right;">“et al.”, “et al.&nbsp;2008”, “et al.&nbsp;(2008)”</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<ol start="4" type="1">
<li>Noise removal. We apply the following cleanup steps: trim white spaces at beginning and end of a sentence, remove the numbers or punctuations at the beginning of a sentence, and remove numbers at the end of a sentence.</li>
</ol>
<p>After the processing, we get a dataset (PMOA-CITE) with approximately 309 million sentences. However, due to the computational cost and in order to make all of our analysis manageable, we randomly sample articles whose sentences produce close to one million sentences. We further split the one million sentences, 60% for training, 20% for validating, and 20% for testing.</p>
</section>
</section>
<section id="text-representation" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="text-representation"><span class="header-section-number">3</span> Text Representation</h2>
<p>Some of our models use different text representations predicting citation worthiness.</p>
<section id="bag-of-words-bow-representation" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="bag-of-words-bow-representation"><span class="header-section-number">3.1</span> Bag of words (BoW) representation</h3>
<p>We follow the standard definition of term-frequency inverse term-frequency (tf-idf) to construct our bag of words (BoW) representation (Manning et al.&nbsp;2008). Our BoW representation for a sentence S which consists of n words will therefore be the vector of all tf-idf values in document <span class="math inline">D_{i}</span>. <span id="eq-bow"><span class="math display">
\text{BoW}(S)=[\text{tf-idf}_{w_{1},D_{i}},...,\text{tf-idf}_{w_{n},D_{i}}]
\tag{1}</span></span></p>
</section>
<section id="topic-modeling-based-tm-representation" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="topic-modeling-based-tm-representation"><span class="header-section-number">3.2</span> Topic modeling based (TM) representation</h3>
<p>Topic modeling is a machine learning technique whose goal is to represent a document as a mixture of a small number of “topics”. This reduces the dimensionality needed to represent a document compared to bag-of-words. There are several topic models available including Latent Semantic Analysis (LSA) and Non-negative Matrix Factorization (NMF). In this paper, we use Latent Dirichlet Allocation (LDA), which is one of the most popular and well-motivated approaches.</p>
</section>
<section id="distributed-word-representation" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="distributed-word-representation"><span class="header-section-number">3.3</span> Distributed word representation</h3>
<p>While topic models can extract statistical structure across documents, they do a relatively poor job at extracting information within documents. In particular, topic models are not meant to find contextual relationships between words. Word embedding methods, in contrast, are based on the distributional hypothesis which states that words that occur in the same context are likely to have similar meaning (<code>Harris, 1954</code>). The famous statement “you shall know a word by the company it keeps” by <code>Firth (1957)</code> is a concise guideline for word embedding: a word could be represented by means of the words surrounding it. In word embedding, words are represented as fixed-length vectors that attempt to approximate their semantic meaning within a document.</p>
<p>There are several distributed word representation methods but one of the most successful and well-known is GloVe by Pennington et al.&nbsp;(<code>2014</code>). We use GloVe word vectors with 300 dimensions, pre-trained on 6 billion tokens.</p>
</section>
</section>
<section id="an-attentionbased-bilstm-architecture-for-citation-worthiness" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="an-attentionbased-bilstm-architecture-for-citation-worthiness"><span class="header-section-number">4</span> An attention‑based BiLSTM architecture for citation worthiness</h2>
<p>In this section, we describe our new architecture for improving upon the performance of classic statistical learning models presented above. Importantly, these models might neglect some of the interpretability but might pay large performance dividends. Generally, they do not need hand-crafted features. At a high level, the architecture we propose has the following layers (also <a href="#fig-network" class="quarto-xref">Figure&nbsp;1</a>):</p>
<ol type="1">
<li>Character embedding layer: encode every character in a word using a bidirectional LSTM, and get a vector representation of a word.</li>
<li>Word embedding layer: convert the tokens into vectors by using pre-trained vectors.</li>
<li>Encoder layer: use a bidirectional LSTM which captures both the forward and backward information flow.</li>
<li>Attention layer: make use of an attention mechanism to interpolate the hidden states of the encoder (explained below)</li>
<li>Contextual Features layer: obtain the contextual features by combining features of sec- tion, previous sentence, current sentence, and next sentence.</li>
<li>Classifier layer: use a multilayer perceptron to produce the final prediction of citation worthiness.</li>
</ol>
<div id="fig-network" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/att_bilstm.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-network-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The architecture of the proposed attention-based BiLSTM neural network
</figcaption>
</figure>
</div>
</section>
<section id="results" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="results"><span class="header-section-number">5</span> Results</h2>
<p>Please read the paper for details.</p>
</section>
<section id="discussion" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="discussion"><span class="header-section-number">6</span> Discussion</h2>
<p>In this work, we developed methods and a large dataset for improving the detection of citation worthiness. Citation worthiness is an important first step for constructing robust and well-structured arguments in science. It is crucial for determining where sources of ideas should mentioned within a manuscript. Previous research has shown promising results but thanks to our new large dataset and modern deep learning architecture, we were able to achieve significantly good performance. We additionally proposed several techniques to interpret what makes scientists use citations. We uncovered potential issues in citation data and behavior: XML documents not properly tagged, citations in the wrong form, and, even worse, scientists failing to cite when they should have. We make our code and a web-based tool available for the scientific community. Our results and new datasets should contribute to the larger need to complement scientific writing with automated techniques. Taken together, our results suggest that deep learning with modern attention-based mechanisms can be effectively used for citation worthiness. We now describe contributions in the context of other work and potential limitations of our approach.</p>
<p>As an enhancement to the ACL-ARC dataset, we proposed the PMOA-CITE dataset in the hope of facilitating research on the citation worthiness task. This extends the datasets available to the field of bio-medical science. Our improvements are 1) a two orders of mag- nitude increase in data size, 2) a well-structured XML file that is less noisy, and 3) contextual information. This dataset could be potentially used in other citation context-related research, such as text summarization (<code>Chen and Zhuge 2019</code>), or citation recommendation (<code>Huang et al. 2015</code>). Therefore, our contribution goes beyond the application of citation worthiness.</p>
<p>Based on the experiments on PMOA-CITE dataset, the use of contextual features consistently improved the performance. This improvement was independent of the algorithm and text representation used (Tables 4 and 7) . A similar results was reported in He et al. (<code>2010</code>) and Jochim and Schütze (<code>2012</code>)). This suggests that contextual information was key for citation worthiness and other related tasks.</p>
<p>In order to facilitate future research, we made our datasets and models available to the public. The links of the dataset and the code parsing XML files are available at https​://githu​b.com/scios​ci/cite-worth​iness​ . We also built a web-based tool (see Fig. 8) at http://cite-worth​iness​.scien​ceofs​cienc​e.org. This tool might help inform journalist, policy makers, the public to better understand the principles of proper source citation and credit assignment.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{zeng2020,
  author = {Zeng, Tong and E Acuna, Daniel},
  title = {Modeling Citation Worthiness by Using Attention‑based
    Bidirectional Long Short‑term Memory Networks and Interpretable
    Models},
  journal = {Scientometrics},
  volume = {124},
  number = {1},
  pages = {399-428},
  date = {2020-07},
  url = {https://link.springer.com/article/10.1007/s11192-020-03421-9},
  doi = {10.1007/s11192-020-03421-9},
  issn = {0138-9130},
  langid = {en},
  abstract = {Scientist learn early on how to cite scientific sources to
    support their claims. Sometimes, however, scientists have challenges
    determining where a citation should be situated—or, even worse, fail
    to cite a source altogether. Automatically detecting sentences that
    need a citation (i.e., citation worthiness) could solve both of
    these issues, leading to more robust and well-constructed scientific
    arguments. Previous researchers have applied machine learning to
    this task but have used small datasets and models that do not take
    advantage of recent algorithmic developments such as attention
    mechanisms in deep learning. We hypothesize that we can develop
    significantly accurate deep learning architectures that learn from
    large supervised datasets constructed from open access publications.
    In this work, we propose a bidirectional long short-term memory
    network with attention mechanism and contextual information to
    detect sentences that need citations. We also produce a new, large
    dataset (PMOA-CITE) based on PubMed Open Access Subset, which is
    orders of magnitude larger than previous datasets. Our experiments
    show that our architecture achieves state of the art performance on
    the standard ACL-ARC dataset ( F1 = 0.507) and exhibits high
    performance ( F1 = 0.856) on the new PMOA-CITE. Moreover, we show
    that it can transfer learning across these datasets. We further use
    interpretable models to illuminate how specific language is used to
    promote and inhibit citations. We discover that sections and
    surrounding sentences are crucial for our improved predictions. We
    further examined purported mispredictions of the model, and
    uncovered systematic human mistakes in citation behavior and source
    data. This opens the door for our model to check documents during
    pre-submission and pre-archival procedures. We discuss limitations
    of our work and make this new dataset, the code, and a web-based
    tool available to the community.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-zeng2020" class="csl-entry quarto-appendix-citeas" role="listitem">
Zeng, Tong, and Daniel E Acuna. 2020. <span>“Modeling Citation
Worthiness by Using Attention‑based Bidirectional Long Short‑term Memory
Networks and Interpretable Models.”</span> <em>Scientometrics</em> 124
(1): 399–428. <a href="https://doi.org/10.1007/s11192-020-03421-9">https://doi.org/10.1007/s11192-020-03421-9</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../publications/most-resources-linked-in-biomedical-articles-disappear-in-eight-years/index.html" class="pagination-link  aria-label=" dead="" science:="" most="" resources="" linked="" in="" biomedical="" articles="" disappear="" eight="" years"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Dead Science: Most Resources Linked in Biomedical Articles Disappear in Eight Years</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2023, Tong Zeng</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/tong-zeng/tong-zeng.github.io/blob/main/publications/modeling-citation-worthiness /index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>